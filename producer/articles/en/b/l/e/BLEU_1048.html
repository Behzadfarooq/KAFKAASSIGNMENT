<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    		<meta name="keywords" content="BLEU,Bleu,Corpus,Correlation,Geometric mean,METEOR,Machine translation,N-gram,NIST,NIST (metric),Natural language" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (English)" />
		<link rel="copyright" href="../../../COPYING.html" />
    <title>BLEU - Wikipedia, the free encyclopedia</title>
    <style type="text/css">/*<![CDATA[*/ @import "../../../skins/htmldump/main.css"; /*]]>*/</style>
    <link rel="stylesheet" type="text/css" media="print" href="../../../skins/common/commonPrint.css" />
    <!--[if lt IE 5.5000]><style type="text/css">@import "../../../skins/monobook/IE50Fixes.css";</style><![endif]-->
    <!--[if IE 5.5000]><style type="text/css">@import "../../../skins/monobook/IE55Fixes.css";</style><![endif]-->
    <!--[if IE 6]><style type="text/css">@import "../../../skins/monobook/IE60Fixes.css";</style><![endif]-->
    <!--[if IE]><script type="text/javascript" src="../../../skins/common/IEFixes.js"></script>
    <meta http-equiv="imagetoolbar" content="no" /><![endif]-->
    <script type="text/javascript" src="../../../skins/common/wikibits.js"></script>
    <script type="text/javascript" src="../../../skins/htmldump/md5.js"></script>
    <script type="text/javascript" src="../../../skins/htmldump/utf8.js"></script>
    <script type="text/javascript" src="../../../skins/htmldump/lookup.js"></script>
    <script type="text/javascript" src="../../../raw/gen.js"></script>        <style type="text/css">/*<![CDATA[*/
@import "../../../raw/MediaWiki%7ECommon.css";
@import "../../../raw/MediaWiki%7EMonobook.css";
@import "../../../raw/gen.css";
/*]]>*/</style>          </head>
  <body
    class="ns-0">
    <div id="globalWrapper">
      <div id="column-content">
	<div id="content">
	  <a name="top" id="contentTop"></a>
	        <h1 class="firstHeading">BLEU</h1>
	  <div id="bodyContent">
	    <h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
	    <div id="contentSub"></div>
	    	    <div class="usermessage">You have <a href="../../../1/2/7/User_talk%7E127.0.0.1.html" title="User talk:127.0.0.1">new messages</a> (<a href="../../../1/2/7/User_talk%7E127.0.0.1.html" title="User talk:127.0.0.1">last change</a>).</div>	    <!-- start content -->
	    <dl>
<dd><i>This page is about the evaluation metric for machine translation. For other meanings, please see <a href="../../../b/l/e/Bleu.html" title="Bleu">Bleu</a>.</i></dd>
</dl>
<p><b>BLEU</b> (<b>Bilingual Evaluation Understudy</b>) is a method for evaluating the quality of text which has been translated from one <a href="../../../n/a/t/Natural_language.html" title="Natural language">natural language</a> to another using <a href="../../../m/a/c/Machine_translation.html" title="Machine translation">machine translation</a>. BLEU was one of the first <a href="../../../s/o/f/Software_metric.html" title="Software metric">software metrics</a> to report high <a href="../../../c/o/r/Correlation.html" title="Correlation">correlation</a> with human judgements of quality. The metric is currently one of the most popular in the field. The central idea behind the metric is that, "the closer a machine translation is to a professional human translation, the better it is".<span class="reference plainlinksneverexpand" id="ref_Papineni2002a"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002a" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002a" rel="nofollow">[1]</a></sup></span></p>
<p>The metric calculates scores for individual segments, generally <a href="../../../s/e/n/Sentence_%28linguistics%29.html" title="Sentence (linguistics)">sentences</a>, and then averages these scores over the whole <a href="../../../c/o/r/Corpus.html" title="Corpus">corpus</a> in order to reach a final score. It has been shown to correlate highly with human judgements of quality at the corpus level.<span class="reference plainlinksneverexpand" id="ref_Papineni2002b"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002b" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002b" rel="nofollow">[2]</a></sup></span><span class="reference plainlinksneverexpand" id="ref_Coughlin2003a"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Coughlin2003a" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Coughlin2003a" rel="nofollow">[3]</a></sup></span> The quality of translation is indicated as a number between 0 and 1 and is measured as statistical closeness to a given set of good quality human reference translations. Therefore, it does not directly take into account translation intelligibility or grammatical correctness.</p>
<p>The metric works by measuring the <a href="../../../n/-/g/N-gram.html" title="N-gram">n-gram</a> co-occurrence between a given translation and the set of reference translations and then taking the weighted <a href="../../../g/e/o/Geometric_mean.html" title="Geometric mean">geometric mean</a>. BLEU is specifically designed to approximate human judgement on a <a href="../../../c/o/r/Corpus.html" title="Corpus">corpus</a> level and performs badly if used to evaluate the quality of isolated sentences.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Algorithm"><span class="tocnumber">1</span> <span class="toctext">Algorithm</span></a></li>
<li class="toclevel-1"><a href="#Performance"><span class="tocnumber">2</span> <span class="toctext">Performance</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">3</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Notes"><span class="tocnumber">4</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">5</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">6</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<p><script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script><a name="Algorithm" id="Algorithm"></a></p>
<h2><span class="editsection">[<a href="../../../b/l/e/BLEU_1048.html" title="Edit section: Algorithm">edit</a>]</span> <span class="mw-headline">Algorithm</span></h2>
<p>BLEU uses a modified form of <a href="../../../p/r/e/Precision.html" title="Precision">precision</a> to compare a candidate translation against multiple reference translations. The metric modifies simple precision since machine translation systems have been known to generate more words than appear in a reference text. This is illustrated in the following example from Papineni et al. (2002),</p>
<table class="wikitable">
<caption>Example of poor machine translation output with high precision</caption>
<tr>
<td>Candidate</td>
<td>the</td>
<td>the</td>
<td>the</td>
<td>the</td>
<td>the</td>
<td>the</td>
<td>the</td>
</tr>
<tr>
<td>Reference 1</td>
<td>the</td>
<td>cat</td>
<td>is</td>
<td>on</td>
<td>the</td>
<td>mat</td>
</tr>
<tr>
<td>Reference 2</td>
<td>there</td>
<td>is</td>
<td>a</td>
<td>cat</td>
<td>on</td>
<td>the</td>
<td>mat</td>
</tr>
</table>
<p>In this example, the candidate text is given a unigram precision of,</p>
<dl>
<dd><img class='tex' src="../../../math/4/f/9/4f944afd1ea66a5184399245c0358666.png" alt="P = \frac{m}{w_{t}} = \frac{7}{7} = 1" /></dd>
</dl>
<p>Of the seven words in the candidate translation, all of them appear in the reference translations. This presents a problem for a metric, as the candidate translation above is complete nonsense, retaining none of the content of either of the references. The modification that BLEU makes is fairly straightforward.</p>
<p>For each word in the candidate translation, the algorithm takes the maximum total count in the reference translations. Taking the example above, the word 'the' appears twice in reference 1, and once in reference 2. The largest value is taken, in this case '2' as the "maximum reference count".</p>
<p>For each of the words in the candidate translation, the count of the word is compared against the maximum reference count, and the lowest value is taken. In this case, the count of the word 'the' in the candidate translation is '7', while the maximum reference count for the word is '2'. This "modified count" is then divided by the total number of words in the candidate translation. In the above example, the modified unigram precision score would be,</p>
<dl>
<dd><img class='tex' src="../../../math/4/f/3/4f3befac3148f6a8c7dcddf61d799380.png" alt="P = \frac{2}{7}" /></dd>
</dl>
<p>The above method is used to calculate scores for each <span class="texhtml"><i>n</i></span>. The value of <span class="texhtml"><i>n</i></span> which has the "highest correlation with monolingual human judgements"<span class="reference plainlinksneverexpand" id="ref_Papineni2002c"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002c" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002c" rel="nofollow">[4]</a></sup></span> was found to be 4. The unigram scores are found to account for the adequacy of the translation, in other words, how much information is retained in the translation. The longer <span class="texhtml"><i>n</i></span>-gram scores account for the fluency of the translation, or to what extent it reads like "good English".</p>
<p>The modification made to precision does not solve the problem of short translations. Short translations can produce very high precision scores, even using modified precision. An example of a candidate translation for the same references as above might be:</p>
<dl>
<dd>the cat</dd>
</dl>
<p>In this example, the modified unigram precision would be,</p>
<dl>
<dd><img class='tex' src="../../../math/0/d/b/0dba104ced0e0756aea38d25fd1e1d9c.png" alt="P = \frac{1}{1} + \frac{1}{1} = \frac{2}{2}" /></dd>
</dl>
<p>as the word 'the' and the word 'cat' appear once each in the candidate, and the total number of words is two. The modified bigram precision would be <span class="texhtml">1 / 1</span> as the bigram, "the cat" appears once in the candidate. It has been pointed out that precision is usually twinned with <a href="../../../r/e/c/Recall.html" title="Recall">recall</a> to overcome this problem <span class="reference plainlinksneverexpand" id="ref_Papineni2002d"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002d" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002d" rel="nofollow">[5]</a></sup></span>, as the unigram recall of this example would be <span class="texhtml">2 / 6</span> or <span class="texhtml">2 / 7</span>. The problem being that as there are multiple reference translations, a bad translation could easily have an inflated recall, such as a translation which consisted of all the words in each of the references.<span class="reference plainlinksneverexpand" id="ref_Papineni2002e"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002e" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002e" rel="nofollow">[6]</a></sup></span></p>
<p>In order to attempt to overcome the problem of very short candidates, a brevity penalty is employed. The brevity penalty is calculated as <span class="texhtml"><i>r</i> / <i>c</i></span> over the whole corpus, where <span class="texhtml"><i>r</i></span> is the total length of the reference corpus, and <span class="texhtml"><i>c</i></span> is the total length of the translation corpus. The length of the reference corpus is taken as being the sum of the closest length matches between each sentence in the references, and each sentence in the translation. In order to produce a score for the whole corpus, the modified precision scores for the segments are combined using the <a href="../../../g/e/o/Geometric_mean.html" title="Geometric mean">geometric mean</a>.</p>
<p><a name="Performance" id="Performance"></a></p>
<h2><span class="editsection">[<a href="../../../b/l/e/BLEU_1048.html" title="Edit section: Performance">edit</a>]</span> <span class="mw-headline">Performance</span></h2>
<p>BLEU has frequently been reported as correlating well with human judgement,<span class="reference plainlinksneverexpand" id="ref_Papineni2002f"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002f" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Papineni2002f" rel="nofollow">[7]</a></sup></span><span class="reference plainlinksneverexpand" id="ref_Coughlin2003b"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Coughlin2003b" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Coughlin2003b" rel="nofollow">[8]</a></sup></span><span class="reference plainlinksneverexpand" id="ref_Doddington2002a"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Doddington2002a" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Doddington2002a" rel="nofollow">[9]</a></sup></span> and certainly remains a benchmark for any new evaluation metric to beat. There are however a number of criticisms that have been voiced. It has been noted that while in theory capable of evaluating any language, BLEU does not in the present form work on languages without word boundaries.<span class="reference plainlinksneverexpand" id="ref_Denoul2005a"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Denoul2005a" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Denoul2005a" rel="nofollow">[10]</a></sup></span></p>
<p>It has been argued that although BLEU certainly has significant advantages, there is no guarantee that an increase in BLEU score is an indicator of improved translation quality.<span class="reference plainlinksneverexpand" id="ref_Callison2006a"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Callison2006a" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Callison2006a" rel="nofollow">[11]</a></sup></span> As BLEU scores are taken at the corpus level, it is difficult to give a textual example. Nevertheless, they highlight two instances where BLEU seriously underperformed. These were the 2005 <a href="../../../n/i/s/NIST_a44b.html" title="NIST">NIST</a> evaluations<span class="reference plainlinksneverexpand" id="ref_Lee2005a"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Lee2005a" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Lee2005a" rel="nofollow">[12]</a></sup></span> where a number of different machine translation systems were tested, and their study of the <a href="../../../s/y/s/SYSTRAN_8749.html" title="SYSTRAN">SYSTRAN</a> engine versus two engines using <a href="../../../s/t/a/Statistical_machine_translation.html" title="Statistical machine translation">statistical machine translation</a> (SMT) techniques.<span class="reference plainlinksneverexpand" id="ref_Callison2006b"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Callison2006b" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Callison2006b" rel="nofollow">[13]</a></sup></span></p>
<p>In the 2005 NIST evaluation, they report that the scores generated by BLEU failed to correspond to the scores produced in the human evaluations. The system which was ranked highest by the human judges was only ranked 6th by BLEU. In their study, they compared SMT systems with SYSTRAN, a knowledge based system. The scores from BLEU for SYSTRAN were substantially worse than the scores given to SYSTRAN by the human judges. They note that the SMT systems were trained using BLEU minimum error rate training,<span class="reference plainlinksneverexpand" id="ref_Och2004a"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Och2004a" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Och2004a" rel="nofollow">[14]</a></sup></span> and point out that this could be one of the reasons behind the difference. They conclude by recommending that BLEU be used in a more restricted manner, for comparing the results from two similar systems, and for tracking "broad, incremental changes to a single system".<span class="reference plainlinksneverexpand" id="ref_Callison2006c"><sup><a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Callison2006c" class="external autonumber" title="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html#endnote_Callison2006c" rel="nofollow">[15]</a></sup></span></p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="../../../b/l/e/BLEU_1048.html" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="../../../n/i/s/NIST_%28metric%29_7bb9.html" title="NIST (metric)">NIST (metric)</a></li>
<li><a href="../../../m/e/t/METEOR_14f9.html" title="METEOR">METEOR</a></li>
</ul>
<p><a name="Notes" id="Notes"></a></p>
<h2><span class="editsection">[<a href="../../../b/l/e/BLEU_1048.html" title="Edit section: Notes">edit</a>]</span> <span class="mw-headline">Notes</span></h2>
<div class="references-small" style="-moz-column-count:2; column-count:2};">
<ol>
<li><cite id="endnote_Papineni2002a" style="font-style: normal;"><a href="#ref_Papineni2002a" title=""><b>↑</b></a></cite>&#160; Papineni, K., et al. (2002)</li>
<li><cite id="endnote_Papineni2002b" style="font-style: normal;"><a href="#ref_Papineni2002b" title=""><b>↑</b></a></cite>&#160; Papineni, K., et al. (2002)</li>
<li><cite id="endnote_Coughlin2003a" style="font-style: normal;"><a href="#ref_Coughlin2003a" title=""><b>↑</b></a></cite>&#160; Coughlin, D. (2003)</li>
<li><cite id="endnote_Papineni2002c" style="font-style: normal;"><a href="#ref_Papineni2002c" title=""><b>↑</b></a></cite>&#160; Papineni, K., et al. (2002)</li>
<li><cite id="endnote_Papineni2002d" style="font-style: normal;"><a href="#ref_Papineni2002d" title=""><b>↑</b></a></cite>&#160; Papineni, K., et al. (2002)</li>
<li><cite id="endnote_Papineni2002e" style="font-style: normal;"><a href="#ref_Papineni2002e" title=""><b>↑</b></a></cite>&#160; Papineni, K., et al. (2002)</li>
<li><cite id="endnote_Papineni2002e" style="font-style: normal;"><a href="#ref_Papineni2002e" title=""><b>↑</b></a></cite>&#160; Papineni, K., et al. (2002)</li>
<li><cite id="endnote_Coughlin2003b" style="font-style: normal;"><a href="#ref_Coughlin2003b" title=""><b>↑</b></a></cite>&#160; Coughlin, D. (2003)</li>
<li><cite id="endnote_Doddington2002a" style="font-style: normal;"><a href="#ref_Doddington2002a" title=""><b>↑</b></a></cite>&#160; Doddington, G. (2002)</li>
<li><cite id="endnote_Denoul2005a" style="font-style: normal;"><a href="#ref_Denoul2005a" title=""><b>↑</b></a></cite>&#160; Denoul, E. and Lepage, Y. (2005)</li>
<li><cite id="endnote_Callison2006a" style="font-style: normal;"><a href="#ref_Callison2006a" title=""><b>↑</b></a></cite>&#160; Callison-Burch, C., Osborne, M. and Koehn, P. (2006)</li>
<li><cite id="endnote_Lee2005a" style="font-style: normal;"><a href="#ref_Lee2005a" title=""><b>↑</b></a></cite>&#160; Lee, A. and Przybocki, M. (2005)</li>
<li><cite id="endnote_Callison2006b" style="font-style: normal;"><a href="#ref_Callison2006b" title=""><b>↑</b></a></cite>&#160; Callison-Burch, C., Osborne, M. and Koehn, P. (2006)</li>
<li><cite id="endnote_Och2004a" style="font-style: normal;"><a href="#ref_Och2004a" title=""><b>↑</b></a></cite>&#160; Lin, C. and Och, F. (2004)</li>
<li><cite id="endnote_Callison2006c" style="font-style: normal;"><a href="#ref_Callison2006c" title=""><b>↑</b></a></cite>&#160; Callison-Burch, C., Osborne, M. and Koehn, P. (2006)</li>
</ol>
</div>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="../../../b/l/e/BLEU_1048.html" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li>Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. (2002). "BLEU: a method for automatic evaluation of machine translation" in <i>ACL-2002: 40th Annual meeting of the Association for Computational Linguistics</i> pp. 311--318</li>
<li>Callison-Burch, C., Osborne, M. and Koehn, P. (2006) "Re-evaluating the Role of BLEU in Machine Translation Research" in <i>11th Conference of the European Chapter of the Association for Computational Linguistics: EACL 2006</i> pp. 249--256</li>
<li>Doddington, G. (2002) "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics" in <i>Proceedings of the Human Language Technology Conference (HLT), San Diego, CA</i> pp. 128--132</li>
<li>Coughlin, D. (2003) "Correlating Automated and Human Assessments of Machine Translation Quality" in <i>MT Summit IX, New Orleans, USA</i> pp. 23--27</li>
<li>Denoul, E. and Lepage, Y. (2005) "BLEU in characters: towards automatic MT evaluation in languages without word delimiters" in <i>Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing</i> pp. 81--86</li>
<li>Lee, A. and Przybocki, M. (2005) NIST 2005 machine translation evaluation official results</li>
<li>Lin, C. and Och, F. (2004) "Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics" in <i>Proceedings of the 42nd Annual Meeting of the Association of Computational Linguistics</i>.</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="../../../b/l/e/BLEU_1048.html" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://www.nist.gov/speech/tests/mt/resources/scoring.htm" class="external text" title="http://www.nist.gov/speech/tests/mt/resources/scoring.htm" rel="nofollow">NIST MTEval</a> (software to calculate BLEU scores)</li>
</ul>

<!-- 
Pre-expand include size: 24632 bytes
Post-expand include size: 6052 bytes
Template argument size: 1058 bytes
Maximum: 2048000 bytes
-->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org../../../b/l/e/BLEU_1048.html">http://en.wikipedia.org../../../b/l/e/BLEU_1048.html</a>"</div>
	    <div id="catlinks"><p class='catlinks'><a href="../../../c/a/t/Special%7ECategories_101d.html" title="Special:Categories">Category</a>: <span dir='ltr'><a href="../../../e/v/a/Category%7EEvaluation_of_machine_translation_90ea.html" title="Category:Evaluation of machine translation">Evaluation of machine translation</a></span></p></div>	    <!-- end content -->
	    <div class="visualClear"></div>
	  </div>
	</div>
      </div>
      <div id="column-one">
	<div id="p-cactions" class="portlet">
	  <h5>Views</h5>
	  <ul>
	    <li id="ca-nstab-main"
	       class="selected"	       ><a href="../../../b/l/e/BLEU_1048.html">Article</a></li><li id="ca-talk"
	       	       ><a href="../../../b/l/e/Talk%7EBLEU_ac50.html">Discussion</a></li><li id="ca-current"
	       	       ><a href="http://en.wikipedia.org/wiki/BLEU">Current revision</a></li>	  </ul>
	</div>
	<div class="portlet" id="p-logo">
	  <a style="background-image: url(../../../images/wiki-en.png);"
	    href="../../../index.html"
	    title="Main Page"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
		<div class='portlet' id='p-navigation'>
	  <h5>Navigation</h5>
	  <div class='pBody'>
	    <ul>
	    	      <li id="n-Main-page"><a href="../../../index.html">Main page</a></li>
	     	      <li id="n-Contents"><a href="../../../c/o/n/Wikipedia%7EContents_3181.html">Contents</a></li>
	     	      <li id="n-Featured-content"><a href="../../../f/e/a/Wikipedia%7EFeatured_content_24ba.html">Featured content</a></li>
	     	      <li id="n-currentevents"><a href="../../../c/u/r/Portal%7ECurrent_events_bb60.html">Current events</a></li>
	     	    </ul>
	  </div>
	</div>
		<div class='portlet' id='p-interaction'>
	  <h5>interaction</h5>
	  <div class='pBody'>
	    <ul>
	    	      <li id="n-About-Wikipedia"><a href="../../../a/b/o/Wikipedia%7EAbout_8d82.html">About Wikipedia</a></li>
	     	      <li id="n-portal"><a href="../../../c/o/m/Wikipedia%7ECommunity_Portal_6a3c.html">Community portal</a></li>
	     	      <li id="n-contact"><a href="../../../c/o/n/Wikipedia%7EContact_us_afd6.html">Contact us</a></li>
	     	      <li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Fundraising">Make a donation</a></li>
	     	      <li id="n-help"><a href="../../../c/o/n/Help%7EContents_22de.html">Help</a></li>
	     	    </ul>
	  </div>
	</div>
		<div id="p-search" class="portlet">
	  <h5><label for="searchInput">Search</label></h5>
	  <div id="searchBody" class="pBody">
	    <form action="javascript:goToStatic(3)" id="searchform"><div>
	      <input id="searchInput" name="search" type="text"
	        accesskey="f" value="" />
	      <input type='submit' name="go" class="searchButton" id="searchGoButton"
	        value="Go" />
	    </div></form>
	  </div>
	</div>
	<div id="p-lang" class="portlet">
	  <h5>In other languages</h5>
	  <div class="pBody">
	    <ul>
	      	      <li>
	      <a href="../../../../fa/%D8%AC/%D8%A7/%DB%8C/%D8%AC%D8%A7%DB%8C%DA%AF%D8%B2%DB%8C%D9%86_%D8%A7%D8%B1%D8%B2%D8%B4%DB%8C%D8%A7%D8%A8%DB%8C_%D8%AF%D9%88%D8%B2%D8%A8%D8%A7%D9%86%D9%87.html">فارسی</a>
	      </li>
	      	      <li>
	      <a href="../../../../tg/b/l/e/BLEU_1048.html">Тоҷикӣ</a>
	      </li>
	      	    </ul>
	  </div>
	</div>
	      </div><!-- end of the left (by default at least) column -->
      <div class="visualClear"></div>
      <div id="footer">
    <div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="../../../skins/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>	<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="../../../images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>	<ul id="f-list">
	  	  	  <li id="f-credits">This page was last modified 09:19, 29 March 2007 by Wikipedia user <a href="../../../f/r/a/User%7EFrancis_Tyers_a973.html" title="User:Francis Tyers">Francis Tyers</a>. Based on work by Wikipedia user(s) <a href="../../../m/a/n/User%7EMani1_e14b.html" title="User:Mani1">Mani1</a>, <a href="../../../l/i/n/User%7ELing.Nut_3c51.html" title="User:Ling.Nut">Ling.Nut</a>, <a href="../../../c/h/r/User%7EChris_the_speller_fea2.html" title="User:Chris the speller">Chris the speller</a>, <a href="../../../b/o/g/User%7EBogdangiusca_0433.html" title="User:Bogdangiusca">Bogdangiusca</a>, <a href="../../../k/a/r/User%7EKarel_Anthonissen_c666.html" title="User:Karel Anthonissen">Karel Anthonissen</a>, <a href="../../../f/r/o/User%7EFrobenius_6ca9.html" title="User:Frobenius">Frobenius</a>, <a href="../../../f/r/a/User%7EFrancisTyers_1664.html" title="User:FrancisTyers">FrancisTyers</a> and <a href="../../../s/a/n/User%7ESandius_cc69.html" title="User:Sandius">Sandius</a> and Anonymous user(s) of Wikipedia.</li>	  <li id="f-copyright">All text is available under the terms of the <a class='internal' href="../../../t/e/x/Wikipedia%7EText_of_the_GNU_Free_Documentation_License_702a.html" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="../../../c/o/p/Wikipedia%7ECopyrights_92c4.html" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc</a>., a US-registered <a class='internal' href="../../../5/0/1/501%28c%29.html#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="../../../n/o/n/Non-profit_organization.html" title="Non-profit organization">nonprofit</a> <a href="../../../c/h/a/Charitable_organization.html" title="Charitable organization">charity</a>.<br /></li>	  <li id="f-about"><a href="../../../a/b/o/Wikipedia%7EAbout_8d82.html" title="Wikipedia:About">About Wikipedia</a></li>	  <li id="f-disclaimer"><a href="../../../g/e/n/Wikipedia%7EGeneral_disclaimer_3e44.html" title="Wikipedia:General disclaimer">Disclaimers</a></li>	  	</ul>
      </div>
    </div>
  </body>
</html>
