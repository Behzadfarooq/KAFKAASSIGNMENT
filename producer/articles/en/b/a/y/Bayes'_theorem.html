<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    		<meta name="keywords" content="Bayes' theorem,1702,1761,1763,1774,A posteriori,Absolutely continuous,Abuse of notation,Athanasios Papoulis,Bayesian inference,Bayesian network" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (English)" />
		<link rel="copyright" href="../../../COPYING.html" />
    <title>Bayes' theorem - Wikipedia, the free encyclopedia</title>
    <style type="text/css">/*<![CDATA[*/ @import "../../../skins/htmldump/main.css"; /*]]>*/</style>
    <link rel="stylesheet" type="text/css" media="print" href="../../../skins/common/commonPrint.css" />
    <!--[if lt IE 5.5000]><style type="text/css">@import "../../../skins/monobook/IE50Fixes.css";</style><![endif]-->
    <!--[if IE 5.5000]><style type="text/css">@import "../../../skins/monobook/IE55Fixes.css";</style><![endif]-->
    <!--[if IE 6]><style type="text/css">@import "../../../skins/monobook/IE60Fixes.css";</style><![endif]-->
    <!--[if IE]><script type="text/javascript" src="../../../skins/common/IEFixes.js"></script>
    <meta http-equiv="imagetoolbar" content="no" /><![endif]-->
    <script type="text/javascript" src="../../../skins/common/wikibits.js"></script>
    <script type="text/javascript" src="../../../skins/htmldump/md5.js"></script>
    <script type="text/javascript" src="../../../skins/htmldump/utf8.js"></script>
    <script type="text/javascript" src="../../../skins/htmldump/lookup.js"></script>
    <script type="text/javascript" src="../../../raw/gen.js"></script>        <style type="text/css">/*<![CDATA[*/
@import "../../../raw/MediaWiki%7ECommon.css";
@import "../../../raw/MediaWiki%7EMonobook.css";
@import "../../../raw/gen.css";
/*]]>*/</style>          </head>
  <body
    class="ns-0">
    <div id="globalWrapper">
      <div id="column-content">
	<div id="content">
	  <a name="top" id="contentTop"></a>
	        <h1 class="firstHeading">Bayes' theorem</h1>
	  <div id="bodyContent">
	    <h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
	    <div id="contentSub"></div>
	    	    	    <!-- start content -->
	    <p><b>Bayes' theorem</b> (also known as <b>Bayes' rule</b> or <b>Bayes' law</b>) is a result in <a href="../../../p/r/o/Probability_theory.html" title="Probability theory">probability theory</a>, which relates the <a href="../../../c/o/n/Conditional_probability.html" title="Conditional probability">conditional</a> and <a href="../../../c/o/n/Conditional_probability.html" title="Conditional probability">marginal</a> <a href="../../../p/r/o/Probability_distribution.html" title="Probability distribution">probability distributions</a> of <a href="../../../r/a/n/Random_variable.html" title="Random variable">random variables</a>. In some interpretations of <a href="../../../p/r/o/Probability.html" title="Probability">probability</a>, Bayes' theorem tells how to update or revise beliefs in light of new evidence: <i><a href="../../../a/_/p/A_posteriori.html" title="A posteriori">a posteriori</a></i>.</p>
<p>The probability of an <a href="../../../e/v/e/Event_%28probability_theory%29.html" title="Event (probability theory)">event</a> <i>A</i> conditional on another event <i>B</i> is generally different from the probability of <i>B</i> conditional on <i>A</i>. However, there is a definite relationship between the two, and Bayes' theorem is the statement of that relationship.</p>
<p>As a formal <a href="../../../t/h/e/Theorem.html" title="Theorem">theorem</a>, Bayes' theorem is valid in all interpretations of probability. However, <a href="../../../f/r/e/Frequentist.html" title="Frequentist">frequentist</a> and <a href="../../../b/a/y/Bayesian_probability.html" title="Bayesian probability">Bayesian</a> interpretations disagree about the kinds of things to which probabilities should be assigned in applications: frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole; Bayesians assign probabilities to propositions that are uncertain. A consequence is that Bayesians have more frequent occasion to use Bayes' theorem. The articles on <a href="../../../b/a/y/Bayesian_probability.html" title="Bayesian probability">Bayesian probability</a> and <a href="../../../f/r/e/Frequency_probability.html" title="Frequency probability">frequentist probability</a> discuss these debates at greater length.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Statement_of_Bayes.27_theorem"><span class="tocnumber">1</span> <span class="toctext">Statement of Bayes' theorem</span></a></li>
<li class="toclevel-1"><a href="#Derivation_from_conditional_probabilities"><span class="tocnumber">2</span> <span class="toctext">Derivation from conditional probabilities</span></a></li>
<li class="toclevel-1"><a href="#Alternative_forms_of_Bayes.27_theorem"><span class="tocnumber">3</span> <span class="toctext">Alternative forms of Bayes' theorem</span></a>
<ul>
<li class="toclevel-2"><a href="#Bayes.27_theorem_in_terms_of_odds_and_likelihood_ratio"><span class="tocnumber">3.1</span> <span class="toctext">Bayes' theorem in terms of odds and likelihood ratio</span></a></li>
<li class="toclevel-2"><a href="#Bayes.27_theorem_for_probability_densities"><span class="tocnumber">3.2</span> <span class="toctext">Bayes' theorem for probability densities</span></a></li>
<li class="toclevel-2"><a href="#Abstract_Bayes.27_theorem"><span class="tocnumber">3.3</span> <span class="toctext">Abstract Bayes' theorem</span></a></li>
<li class="toclevel-2"><a href="#Extensions_of_Bayes.27_theorem"><span class="tocnumber">3.4</span> <span class="toctext">Extensions of Bayes' theorem</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Examples"><span class="tocnumber">4</span> <span class="toctext">Examples</span></a>
<ul>
<li class="toclevel-2"><a href="#Example_.231:__Conditional_probabilities"><span class="tocnumber">4.1</span> <span class="toctext">Example #1: Conditional probabilities</span></a>
<ul>
<li class="toclevel-3"><a href="#Tables_of_occurrences_and_relative_frequencies"><span class="tocnumber">4.1.1</span> <span class="toctext">Tables of occurrences and relative frequencies</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="#Example_.232:__Drug_testing"><span class="tocnumber">4.2</span> <span class="toctext">Example #2: Drug testing</span></a></li>
<li class="toclevel-2"><a href="#Example_.233:__Bayesian_inference"><span class="tocnumber">4.3</span> <span class="toctext">Example #3: Bayesian inference</span></a></li>
<li class="toclevel-2"><a href="#Example_.234:_The_Monty_Hall_problem"><span class="tocnumber">4.4</span> <span class="toctext">Example #4: The Monty Hall problem</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Historical_remarks"><span class="tocnumber">5</span> <span class="toctext">Historical remarks</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">6</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a>
<ul>
<li class="toclevel-2"><a href="#Versions_of_the_essay"><span class="tocnumber">7.1</span> <span class="toctext">Versions of the essay</span></a></li>
<li class="toclevel-2"><a href="#Commentaries"><span class="tocnumber">7.2</span> <span class="toctext">Commentaries</span></a></li>
<li class="toclevel-2"><a href="#Additional_material"><span class="tocnumber">7.3</span> <span class="toctext">Additional material</span></a></li>
</ul>
</li>
</ul>
</td>
</tr>
</table>
<p><script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script><a name="Statement_of_Bayes.27_theorem" id="Statement_of_Bayes.27_theorem"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Statement of Bayes' theorem">edit</a>]</span> <span class="mw-headline">Statement of Bayes' theorem</span></h2>
<p>Bayes' theorem relates the conditional and marginal probabilities of <a href="../../../s/t/o/Stochastic_process.html" title="Stochastic process">stochastic</a> <a href="../../../e/v/e/Event_%28probability_theory%29.html" title="Event (probability theory)">events</a> <i>A</i> and <i>B</i>:</p>
<dl>
<dd><img class='tex' src="../../../math/3/f/0/3f024a0d2eba7ea94aa6b2bb1e73ed69.png" alt="\begin{align}   \Pr(A|B)  &amp;  =       \frac{\Pr(B | A)\, \Pr(A)}{\Pr(B)}  \\             &amp;  \propto L(A | B)\, \Pr(A)   \end{align}" /></dd>
</dl>
<p>where <i>L</i>(<i>A</i>|<i>B</i>) is the <a href="../../../l/i/k/Likelihood.html" title="Likelihood">likelihood</a> of <i>A</i> given fixed <i>B</i>. Notice the relationship <img class='tex' src="../../../math/f/b/8/fb803e0ffc0a7cc6f99a3743e4cea9e5.png" alt="\Pr(B | A) = L(A | B)" />.</p>
<p>Each term in Bayes' theorem has a conventional name:</p>
<ul>
<li>Pr(<i>A</i>) is the <a href="../../../p/r/i/Prior_probability.html" title="Prior probability">prior probability</a> or <a href="../../../m/a/r/Marginal_probability.html" title="Marginal probability">marginal probability</a> of <i>A</i>. It is "prior" in the sense that it does not take into account any information about <i>B</i>.</li>
<li>Pr(<i>A</i>|<i>B</i>) is the <a href="../../../c/o/n/Conditional_probability.html" title="Conditional probability">conditional probability</a> of <i>A</i>, given <i>B</i>. It is also called the <a href="../../../p/o/s/Posterior_probability.html" title="Posterior probability">posterior probability</a> because it is derived from or depends upon the specified value of <i>B</i>.</li>
<li>Pr(<i>B</i>|<i>A</i>) is the conditional probability of <i>B</i> given <i>A</i>.</li>
<li>Pr(<i>B</i>) is the prior or marginal probability of <i>B</i>, and acts as a <a href="../../../n/o/r/Normalizing_constant.html" title="Normalizing constant">normalizing constant</a>.</li>
</ul>
<p>With this terminology, the theorem may be paraphrased as</p>
<dl>
<dd><img class='tex' src="../../../math/3/e/2/3e294d8052aa226062a1a1158be30079.png" alt="\mbox{posterior} = \frac{\mbox{likelihood} \times \mbox{prior}} {\mbox{normalizing constant}}" /></dd>
</dl>
<p>In words: the posterior probability is proportional to the prior probability times the likelihood.</p>
<p>In addition, the ratio Pr(<i>B</i>|<i>A</i>)/Pr(<i>B</i>) is sometimes called the standardised likelihood, so the theorem may also be paraphrased as</p>
<dl>
<dd><img class='tex' src="../../../math/9/c/1/9c1f50e099b3ae7e1ece4cb35ca21356.png" alt="\mbox{posterior} = {\mbox{standardised likelihood} \times \mbox{prior} }.\," /></dd>
</dl>
<p><a name="Derivation_from_conditional_probabilities" id="Derivation_from_conditional_probabilities"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Derivation from conditional probabilities">edit</a>]</span> <span class="mw-headline">Derivation from conditional probabilities</span></h2>
<p>To derive the theorem, we start from the definition of <a href="../../../c/o/n/Conditional_probability.html" title="Conditional probability">conditional probability</a>. The probability of event <i>A</i> given event <i>B</i> is</p>
<dl>
<dd><img class='tex' src="../../../math/3/6/5/365fe01a91eddd5efe843b6c17985e96.png" alt="\Pr(A|B)=\frac{\Pr(A \cap B)}{\Pr(B)}." /></dd>
</dl>
<p>Likewise, the probability of event <i>B</i> given event <i>A</i> is</p>
<dl>
<dd><img class='tex' src="../../../math/f/9/9/f9970e3bd225504bc5c54733bd8e78d2.png" alt="\Pr(B|A) = \frac{\Pr(A \cap B)}{\Pr(A)}. \!" /></dd>
</dl>
<p>Rearranging and combining these two equations, we find</p>
<dl>
<dd><img class='tex' src="../../../math/8/4/5/845403665eb223ca48d2792d0d9d132b.png" alt="\Pr(A|B)\, \Pr(B) = \Pr(A \cap B) = \Pr(B|A)\, \Pr(A). \!" /></dd>
</dl>
<p>This <a href="../../../l/e/m/Lemma_%28mathematics%29.html" title="Lemma (mathematics)">lemma</a> is sometimes called the product rule for probabilities. Dividing both sides by Pr(<i>B</i>), providing that it is non-zero, we obtain Bayes' theorem:</p>
<dl>
<dd><img class='tex' src="../../../math/b/2/6/b26222db003c783d5a92815732533810.png" alt="\Pr(A|B) = \frac{\Pr(B|A)\,\Pr(A)}{\Pr(B)}. \!" /></dd>
</dl>
<p><a name="Alternative_forms_of_Bayes.27_theorem" id="Alternative_forms_of_Bayes.27_theorem"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Alternative forms of Bayes' theorem">edit</a>]</span> <span class="mw-headline">Alternative forms of Bayes' theorem</span></h2>
<p>Bayes' theorem is often embellished by noting that</p>
<dl>
<dd><img class='tex' src="../../../math/0/d/8/0d89469120dc43a2d6f3619460442ba2.png" alt="\Pr(B) = \Pr(A\cap B) + \Pr(A^C\cap B) = \Pr(B|A) \Pr(A) + \Pr(B|A^C) \Pr(A^C)\," /></dd>
</dl>
<p>where <i>A</i><sup><i>C</i></sup> is the <a href="../../../c/o/m/Complement_%28set_theory%29.html#Absolute_complement" title="Complement (set theory)">complementary</a> event of <i>A</i> (often called "not A"). So the theorem can be restated as</p>
<dl>
<dd><img class='tex' src="../../../math/9/1/4/9140dff6d6e49a61d296a6893c7c4ba9.png" alt="\Pr(A|B) = \frac{\Pr(B | A)\, \Pr(A)}{\Pr(B|A)\Pr(A) + \Pr(B|A^C)\Pr(A^C)}.  \!" /></dd>
</dl>
<p>More generally, where {<i>A</i><sub><i>i</i></sub>} forms a <a href="../../../p/a/r/Partition_of_a_set.html" title="Partition of a set">partition</a> of the event space,</p>
<dl>
<dd><img class='tex' src="../../../math/0/e/e/0eefcf18d91289747cb162e5ee3b530f.png" alt="\Pr(A_i|B) = \frac{\Pr(B | A_i)\, \Pr(A_i)}{\sum_j \Pr(B|A_j)\,\Pr(A_j)} , \!" /></dd>
</dl>
<p>for any <i>A</i><sub><i>i</i></sub> in the partition.</p>
<p>See also the <a href="../../../l/a/w/Law_of_total_probability.html" title="Law of total probability">law of total probability</a>.</p>
<p><a name="Bayes.27_theorem_in_terms_of_odds_and_likelihood_ratio" id="Bayes.27_theorem_in_terms_of_odds_and_likelihood_ratio"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Bayes' theorem in terms of odds and likelihood ratio">edit</a>]</span> <span class="mw-headline">Bayes' theorem in terms of odds and likelihood ratio</span></h3>
<p>Bayes' theorem can also be written neatly in terms of a <a href="../../../l/i/k/Likelihood_function.html" title="Likelihood function">likelihood</a> ratio Λ and <a href="../../../o/d/d/Odds.html" title="Odds">odds</a> <i>O</i> as</p>
<dl>
<dd><img class='tex' src="../../../math/6/a/a/6aa65315e6ae2da1b417789e43b4eb72.png" alt="O(A|B)=O(A) \cdot \Lambda (A|B)" /></dd>
</dl>
<p>where <img class='tex' src="../../../math/b/a/0/ba0979860ec4bb0313de55fc8b9c4ba2.png" alt="O(A|B)=\frac{\Pr(A|B)}{\Pr(A^C|B)} \!" /> are the <i>odds</i> of <i>A</i> given <i>B</i>,</p>
<p>and <img class='tex' src="../../../math/3/2/a/32a871b266cc6de3967e4cd8ff629c5c.png" alt="O(A)=\frac{\Pr(A)}{\Pr(A^C)} \!" /> are the odds of <i>A</i> by itself,</p>
<p>while <img class='tex' src="../../../math/9/6/8/9687f1454a5652c46dde2b9a50c8cee7.png" alt="\Lambda (A|B) = \frac{L(A|B)}{L(A^C|B)} = \frac{\Pr(B|A)}{\Pr(B|A^C)} \!" /> is the likelihood ratio.</p>
<p><a name="Bayes.27_theorem_for_probability_densities" id="Bayes.27_theorem_for_probability_densities"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Bayes' theorem for probability densities">edit</a>]</span> <span class="mw-headline">Bayes' theorem for probability densities</span></h3>
<p>There is also a version of Bayes' theorem for continuous distributions. It is somewhat harder to derive, since probability densities, strictly speaking, are not probabilities, so Bayes' theorem has to be established by a limit process; see Papoulis (citation below), Section 7.3 for an elementary derivation. Bayes's theorem for probability densities is formally similar to the theorem for probabilities:</p>
<dl>
<dd><img class='tex' src="../../../math/4/0/2/402214a6b5fb1babe545afc206a96d92.png" alt="f(x|y) = \frac{f(x,y)}{f(y)} = \frac{f(y|x)\,f(x)}{f(y)} \!" /></dd>
</dl>
<p>and there is an analogous statement of the law of total probability:</p>
<dl>
<dd><img class='tex' src="../../../math/9/0/8/908da7e26f756ae79f5ac0bfce87c5ce.png" alt="f(x|y) = \frac{f(y|x)\,f(x)}{\int_{-\infty}^{\infty} f(y|x)\,f(x)\,dx}. \!" /></dd>
</dl>
<p>As in the discrete case, the terms have standard names. <i>f</i>(<i>x</i>, <i>y</i>) is the joint distribution of <i>X</i> and <i>Y</i>, <i>f</i>(<i>x</i>|<i>y</i>) is the posterior distribution of <i>X</i> given <i>Y</i>=<i>y</i>, <i>f</i>(<i>y</i>|<i>x</i>)&#160;=&#160;<i>L</i>(<i>x</i>|<i>y</i>) is (as a function of <i>x</i>) the likelihood function of <i>X</i> given <i>Y</i>=<i>y</i>, and <i>f</i>(<i>x</i>) and <i>f</i>(<i>y</i>) are the marginal distributions of <i>X</i> and <i>Y</i> respectively, with <i>f</i>(<i>x</i>) being the prior distribution of <i>X</i>.</p>
<p>Here we have indulged in a conventional <a href="../../../a/b/u/Abuse_of_notation.html" title="Abuse of notation">abuse of notation</a>, using <i>f</i> for each one of these terms, although each one is really a different function; the functions are distinguished by the names of their arguments.</p>
<p><a name="Abstract_Bayes.27_theorem" id="Abstract_Bayes.27_theorem"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Abstract Bayes' theorem">edit</a>]</span> <span class="mw-headline">Abstract Bayes' theorem</span></h3>
<p>Given two <a href="../../../a/b/s/Absolutely_continuous.html" title="Absolutely continuous">absolutely continuous</a> probability measures <span class="texhtml"><i>P</i>˜<i>Q</i></span> on the probability space <img class='tex' src="../../../math/b/9/4/b9474d45d82accf03434710c10871795.png" alt="(\Omega, \mathcal{F})" /> and a sigma-algebra <img class='tex' src="../../../math/6/9/5/6956beed6709f29c47056603dd448e37.png" alt="\mathcal{G} \subset \mathcal{F}" />, the abstract Bayes theorem for a <img class='tex' src="../../../math/2/6/a/26afd73f8c17f310707120691ccc4a35.png" alt="\mathcal{F}" />-measurable random variable <span class="texhtml"><i>X</i></span> becomes</p>
<dl>
<dd><img class='tex' src="../../../math/b/c/b/bcb58d4f262347072d35dea4a65977dd.png" alt="E_P[X|\mathcal{G}] = \frac{E_Q[\frac{dP}{dQ} X |\mathcal{G}]}{E_Q[\frac{dP}{dQ}|\mathcal{G}]}" />.</dd>
</dl>
<p>This formulation is used in <a href="../../../k/a/l/Kalman_filtering.html" title="Kalman filtering">Kalman filtering</a> to find <a href="../../../z/a/k/Zakai_equation.html" title="Zakai equation">Zakai equations</a>. It is also used in <a href="../../../f/i/n/Financial_mathematics.html" title="Financial mathematics">financial mathematics</a> for change of <a href="../../../n/u/m/Numeraire.html" title="Numeraire">numeraire</a> techniques.</p>
<p><a name="Extensions_of_Bayes.27_theorem" id="Extensions_of_Bayes.27_theorem"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Extensions of Bayes' theorem">edit</a>]</span> <span class="mw-headline">Extensions of Bayes' theorem</span></h3>
<p>Theorems analogous to Bayes' theorem hold in problems with more than two variables. For example:</p>
<dl>
<dd><img class='tex' src="../../../math/d/5/0/d507dd87a1d0ecc61996c5f9d1a2186f.png" alt="\Pr(A|B,C) = \frac{\Pr(A) \, \Pr(B|A) \, \Pr(C|A,B)}{\Pr(B) \, \Pr(C|B)} \!" /></dd>
</dl>
<p>This can be derived in several steps from Bayes' theorem and the definition of conditional probability:</p>
<dl>
<dd><img class='tex' src="../../../math/a/2/3/a230e574726b703dbae74af96c2f2731.png" alt="\Pr(A|B,C) = \frac{\Pr(A,B,C)}{\Pr(B,C)} = \frac{\Pr(A,B,C)}{\Pr(B) \, \Pr(C|B)} =" /></dd>
<dd><img class='tex' src="../../../math/7/c/2/7c21f23ab0d44c0a0768e129ec9d5c89.png" alt="= \frac{\Pr(C|A,B) \, \Pr(A,B)}{\Pr(B) \, \Pr(C|B)} = \frac{\Pr(A) \, \Pr(B|A) \, \Pr(C|A,B)}{\Pr(B) \, \Pr(C|B)} ." /></dd>
</dl>
<p>A general strategy is to work with a decomposition of the <a href="../../../j/o/i/Joint_probability.html" title="Joint probability">joint probability</a>, and to marginalize (integrate) over the variables that are not of interest. Depending on the form of the decomposition, it may be possible to prove that some integrals must be 1, and thus they fall out of the decomposition; exploiting this property can reduce the computations very substantially. A <a href="../../../b/a/y/Bayesian_network.html" title="Bayesian network">Bayesian network</a>, for example, specifies a factorization of a <a href="../../../j/o/i/Joint_distribution.html" title="Joint distribution">joint distribution</a> of several variables in which the conditional probability of any one variable given the remaining ones takes a particularly simple form (see <a href="../../../m/a/r/Markov_blanket.html" title="Markov blanket">Markov blanket</a>).</p>
<p><a name="Examples" id="Examples"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Examples">edit</a>]</span> <span class="mw-headline">Examples</span></h2>
<p><a name="Example_.231:__Conditional_probabilities" id="Example_.231:__Conditional_probabilities"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Example #1:  Conditional probabilities">edit</a>]</span> <span class="mw-headline">Example #1: Conditional probabilities</span></h3>
<p>Suppose there are two bowls full of cookies. Bowl #1 has 10 chocolate chip cookies and 30 plain cookies, while bowl #2 has 20 of each. Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?</p>
<p>Intuitively, it seems clear that the answer should be more than a half, since there are more plain cookies in bowl #1. The precise answer is given by Bayes' theorem. But first, we can clarify the situation by rephrasing the question to "what’s the probability that Fred picked bowl #1, given that he has a plain cookie?” Thus, to relate to our previous explanation, the event <i>A</i> is that Fred picked bowl #1, and the event <i>B</i> is that Fred picked a plain cookie. To compute Pr(<i>A</i>|<i>B</i>), we first need to know:</p>
<ul>
<li>Pr(<i>A</i>), or the probability that Fred picked bowl #1 regardless of any other information. Since Fred is treating both bowls equally, it is 0.5.</li>
<li>Pr(<i>B</i>), or the probability of getting a plain cookie regardless of any information on the bowls. In other words, this is the probability of getting a plain cookie from each of the bowls. It is computed as the sum of the probability of getting a plain cookie from a bowl multiplied by the probability of selecting this bowl. We know from the problem statement that the probability of getting a plain cookie from bowl #1 is 0.75, and the probability of getting one from bowl #2 is 0.5, and since Fred is treating both bowls equally the probability of selecting any one of them is 0.5. Thus, the probability of getting a plain cookie overall is 0.75×0.5&#160;+&#160;0.5×0.5 = 0.625. Or to put it more simply, the proportion of all cookies that are plain is 50 out of 80 = 50/80 = 0.625.</li>
<li>Pr(<i>B</i>|<i>A</i>), or the probability of getting a plain cookie given that Fred has selected bowl #1. From the problem statement, we know this is 0.75, since 30 out of 40 cookies in bowl #1 are plain.</li>
</ul>
<p>Given all this information, we can compute the probability of Fred having selected bowl #1 given that he got a plain cookie, as such:</p>
<dl>
<dd><img class='tex' src="../../../math/4/5/2/452f938365c79488c71e3a06e0ef88b1.png" alt="\Pr(A|B) = \frac{\Pr(B | A) \Pr(A)}{\Pr(B)} = \frac{0.75 \times 0.5}{0.625} = 0.6" /></dd>
</dl>
<p>As we expected, it is more than half.</p>
<p><a name="Tables_of_occurrences_and_relative_frequencies" id="Tables_of_occurrences_and_relative_frequencies"></a></p>
<h4><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Tables of occurrences and relative frequencies">edit</a>]</span> <span class="mw-headline">Tables of occurrences and relative frequencies</span></h4>
<p>It is often helpful when calculating conditional probabilities to create a simple table containing the number of occurrences of each outcome, or the <a href="../../../r/e/l/Relative_frequency.html" title="Relative frequency">relative frequencies</a> of each outcome, for each of the independent variables. The tables below illustrate the use of this method for the cookies.</p>
<table>
<tr>
<th>Number of cookies in each bowl<br />
by type of cookie</th>
<th>&#160; &#160; &#160; &#160; &#160;</th>
<th>Relative frequency of cookies in each bowl<br />
by type of cookie</th>
</tr>
<tr>
<td>
<table class="wikitable">
<tr>
<th></th>
<th>Bowl #1</th>
<th>Bowl #2</th>
<th>Totals</th>
</tr>
<tr>
<td>Chocolate Chip</td>
<td>
<center>10</center>
</td>
<td>
<center>20</center>
</td>
<td>
<center>30</center>
</td>
</tr>
<tr>
<td>Plain</td>
<td>
<center>30</center>
</td>
<td>
<center>20</center>
</td>
<td>
<center>50</center>
</td>
</tr>
<tr>
<td>Total</td>
<td>
<center>40</center>
</td>
<td>
<center>40</center>
</td>
<td>
<center>80</center>
</td>
</tr>
</table>
</td>
<td></td>
<td>
<table class="wikitable">
<tr>
<th></th>
<th>Bowl #1</th>
<th>Bowl #2</th>
<th>Totals</th>
</tr>
<tr>
<td>Chocolate Chip</td>
<td>
<center>0.125</center>
</td>
<td>
<center>0.250</center>
</td>
<td>
<center>0.375</center>
</td>
</tr>
<tr>
<td>Plain</td>
<td>
<center>0.375</center>
</td>
<td>
<center>0.250</center>
</td>
<td>
<center>0.625</center>
</td>
</tr>
<tr>
<td>Total</td>
<td>
<center>0.500</center>
</td>
<td>
<center>0.500</center>
</td>
<td>
<center>1.000</center>
</td>
</tr>
</table>
</td>
</tr>
</table>
<p>The table on the right is derived from the table on the left by dividing each entry by the total number of cookies under consideration, i.e. dividing each number by 80.</p>
<p><a name="Example_.232:__Drug_testing" id="Example_.232:__Drug_testing"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Example #2:  Drug testing">edit</a>]</span> <span class="mw-headline">Example #2: Drug testing</span></h3>
<p>Bayes' theorem is useful in evaluating the result of <a href="../../../d/r/u/Drug_test.html" title="Drug test">drug tests</a>. Suppose a certain drug test is 99% accurate, that is, the test will correctly identify a drug user as testing positive 99% of the time, and will correctly identify a non-user as testing negative 99% of the time. This would seem to be a relatively accurate test, but Bayes' theorem will reveal a potential flaw. Let's assume a corporation decides to test its employees for <a href="../../../o/p/i/Opium.html" title="Opium">opium</a> use, and 0.5% of the employees use the drug. We want to know the <a href="../../../p/r/o/Probability.html" title="Probability">probability</a> that, given a positive drug test, an employee is actually a drug user. Let "D" be the event of being a drug user and "N" indicate being a non-user. Let "+" be the event of a positive drug test. We need to know the following:</p>
<ul>
<li>Pr(<i>D</i>), or the probability that the employee is a drug user, regardless of any other information. This is 0.005, since 0.5% of the employees are drug users.</li>
<li>Pr(<i>N</i>), or the probability that the employee is not a drug user. This is 1-Pr(D), or 0.995.</li>
<li>Pr(<i>+</i>|<i>D</i>), or the probability that the test is positive, given that the employee is a drug user. This is 0.99, since the test is 99% accurate.</li>
<li>Pr(<i>+</i>|<i>N</i>), or the probability that the test is positive, given that the employee is not a drug user. This is 0.01, since the test will produce a <a href="../../../f/a/l/False_positive.html" title="False positive">false positive</a> for 1% of non-users.</li>
<li>Pr(<i>+</i>), or the probability of a positive test event, regardless of other information. This is 0.015 or 1.5%, which found by adding the probability that the test will produce a true positive result in the event of drug use (= 99% x 0.5% = 0.495%) plus the probability that the test will produce a false positive in the event of non-drug use (= 1% x 99.5% = 0.995%).</li>
</ul>
<p>Given this information, we can compute the probability that an employee who tested positive is actually a drug user:</p>
<dl>
<dd><img class='tex' src="../../../math/f/e/5/fe544b5ce4521f0fe91b97f2d261d99c.png" alt="\begin{align}\Pr(D|+) &amp; = \frac{\Pr(+ | D) \Pr(D)}{\Pr(+)} \\ &amp; = \frac{\Pr(+ | D) \Pr(D)}{\Pr(+ | D) \Pr(D) + \Pr(+ | N) \Pr(N)} \\ &amp; = \frac{0.99 \times 0.005}{0.99 \times 0.005 + 0.01 \times 0.995} \\ &amp; = 0.3322\end{align}" /></dd>
</dl>
<p>Despite the high accuracy of the test, the probability that the employee is actually a drug user is only about 33%. The rarer the condition for which we are testing, the greater percentage of the positive tests will be false positives. This illustrates why it is important to do follow-up tests.</p>
<p><a name="Example_.233:__Bayesian_inference" id="Example_.233:__Bayesian_inference"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Example #3:  Bayesian inference">edit</a>]</span> <span class="mw-headline">Example #3: Bayesian inference</span></h3>
<p>Applications of Bayes' theorem often assume the philosophy underlying <a href="../../../b/a/y/Bayesian_probability.html" title="Bayesian probability">Bayesian probability</a> that uncertainty and degrees of belief can be measured as probabilities. One such example follows. For additional worked out examples, including simpler examples, please see the article on the examples of <a href="../../../b/a/y/Bayesian_inference.html#Simple_examples_of_Bayesian_inference" title="Bayesian inference">Bayesian inference</a>.</p>
<p>We describe the marginal probability distribution of a variable <i>A</i> as the <i><a href="../../../p/r/i/Prior_probability_distribution.html" title="Prior probability distribution">prior probability distribution</a></i> or simply the <i>prior</i>. The conditional distribution of <i>A</i> given the "data" <i>B</i> is the <i><a href="../../../p/o/s/Posterior_probability_distribution.html" title="Posterior probability distribution">posterior probability distribution</a></i> or just the <i>posterior</i>.</p>
<p>Suppose we wish to know about the proportion <b>r</b> of voters in a large population who will vote "yes" in a referendum. Let <b>n</b> be the number of voters in a random sample (chosen with replacement, so that we have <a href="../../../s/t/a/Statistical_independence.html" title="Statistical independence">statistical independence</a>) and let <b>m</b> be the number of voters in that random sample who will vote "yes". Suppose that we observe <i>n</i>&#160;=&#160;10 voters and <i>m</i>&#160;=&#160;7 say they will vote yes. From Bayes' theorem we can calculate the probability distribution function for <i>r</i> using</p>
<dl>
<dd><img class='tex' src="../../../math/8/1/d/81dbc60f7af3163c790e7891c072ed53.png" alt="f(r | n=10, m=7) =    \frac {f(m=7 | r, n=10) \, f(r)} {\int_0^1 f(m=7|r, n=10) \, f(r) \, dr}. \!" /></dd>
</dl>
<p>From this we see that from the prior probability density function <i>f</i>(<i>r</i>) and the likelihood function <i>L</i>(<i>r</i>)&#160;=&#160;<i>f</i>(<i>m</i>&#160;=&#160;7|<i>r</i>, <i>n</i>&#160;=&#160;10), we can compute the posterior probability density function <i>f</i>(<i>r</i>|<i>n</i>&#160;=&#160;10, <i>m</i>&#160;=&#160;7).</p>
<p>The prior probability density function <i>f</i>(<i>r</i>) summarizes what we know about the distribution of <i>r</i> in the absence of any observation. We provisionally assume in this case that the prior distribution of <i>r</i> is uniform over the interval [0, 1]. That is, <i>f</i>(<i>r</i>) = 1. If some additional background information is found, we should modify the prior accordingly. However before we have any observations, all outcomes are equally likely.</p>
<p>Under the assumption of random sampling, choosing voters is just like choosing balls from an urn. The likelihood function <i>L</i>(<i>r</i>)&#160;=&#160;<i>P</i>(<i>m</i>&#160;=&#160;7|<i>r</i>, <i>n</i>&#160;=&#160;10,) for such a problem is just the probability of 7 successes in 10 trials for a <a href="../../../b/i/n/Binomial_distribution.html" title="Binomial distribution">binomial distribution</a>.</p>
<dl>
<dd><img class='tex' src="../../../math/0/a/a/0aaeea3bb076a54c14036ef53e515b41.png" alt="\Pr( m=7 | r, n=10) = {10 \choose 7} \, r^7 \, (1-r)^3." /></dd>
</dl>
<p>As with the prior, the likelihood is open to revision -- more complex assumptions will yield more complex likelihood functions. Maintaining the current assumptions, we compute the normalizing factor,</p>
<dl>
<dd><img class='tex' src="../../../math/f/6/b/f6b03ec6b8fe850f1c76e3a128cedb68.png" alt="\int_0^1 \Pr( m=7|r, n=10) \, f(r) \, dr = \int_0^1 {10 \choose 7} \, r^7 \, (1-r)^3 \, 1 \, dr = {10 \choose 7} \, \frac{1}{1320} \!" /></dd>
</dl>
<p>and the posterior distribution for <i>r</i> is then</p>
<dl>
<dd><img class='tex' src="../../../math/c/5/9/c5932ce24d2e109050d0846a541a70b1.png" alt="f(r | n=10, m=7) =   \frac{{10 \choose 7} \, r^7 \, (1-r)^3 \, 1} {{10 \choose 7} \, \frac{1}{1320}} = 1320 \, r^7 \, (1-r)^3" /></dd>
</dl>
<p>for <i>r</i> between 0 and 1, inclusive.</p>
<p>One may be interested in the probability that more than half the voters will vote "yes". The <i>prior probability</i> that more than half the voters will vote "yes" is 1/2, by the symmetry of the <a href="../../../u/n/i/Uniform_distribution.html" title="Uniform distribution">uniform distribution</a>. In comparison, the posterior probability that more than half the voters will vote "yes", i.e., the conditional probability given the outcome of the opinion poll – that seven of the 10 voters questioned will vote "yes" – is</p>
<dl>
<dd><img class='tex' src="../../../math/d/c/5/dc58f3b5ce30cd67b381fb124e19c384.png" alt="1320\int_{1/2}^1 r^7(1-r)^3\,dr \approx 0.887, \!" /></dd>
</dl>
<p>which is about an "89% chance".</p>
<p><a name="Example_.234:_The_Monty_Hall_problem" id="Example_.234:_The_Monty_Hall_problem"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Example #4: The Monty Hall problem">edit</a>]</span> <span class="mw-headline">Example #4: The <a href="../../../m/o/n/Monty_Hall_problem_4d49.html" title="Monty Hall problem">Monty Hall problem</a></span></h3>
<p>We are presented with three doors - red, green, and blue - one of which has a prize. We choose the <b>red</b> door. A presenter <i>who knows what door the prize is behind, and who must open a door, but is not permitted to open the door we have picked or the door with the prize</i>, opens the <i>green</i> door and reveals that there is no prize behind it. What is the probability that the prize is behind the blue door?</p>
<p>Let us call the the situation that the prize is behind a given door <b>A<sub>r</sub></b>, <b>A<sub>g</sub></b>, and <b>A<sub>b</sub></b>.</p>
<p>To start with, <img class='tex' src="../../../math/c/a/6/ca6dbefbcd39eef6b01f6e7fbd8f8bd0.png" alt="\Pr(A_r) = \Pr(A_g) = \Pr(A_b) = \frac 1 3" />, and to make things simpler we shall assume that we have already picked the red door.</p>
<p>Let us call <b>B</b> "the presenter opens the green door". Without any prior knowledge, we would assign this a value of 50%</p>
<ul>
<li>In the situation where the prize is behind the red door, the host is free to pick the green or blue door random. Thus, <img class='tex' src="../../../math/7/8/5/7850e89d82dd18f7a7a8552e203dd740.png" alt="\Pr(B|A_r) = 1/2" /></li>
<li>In the situation where the prize is behind the green door, the host must pick the blue door. Thus, <img class='tex' src="../../../math/8/3/d/83d15b4adac8d3cf7e432b52c80bacb0.png" alt="\Pr(B|A_g) = 0" /></li>
<li>In the situation where the prize is behind the blue door, the host must pick the green door. Thus, <img class='tex' src="../../../math/f/8/a/f8a74a76d275bf3f8657370c216747ae.png" alt="\Pr(B|A_b) = 1" /></li>
</ul>
<p>Thus,</p>
<p><img class='tex' src="../../../math/b/4/7/b476e069be7f60ba2d81455786049a37.png" alt="\begin{matrix}   \Pr(A_r|B) &amp; =  \frac{\Pr(B | A_r) \Pr(A_r)}{\Pr(B)} &amp; =   \frac{\frac 1 2 \frac 1 3}{\frac 1 2} &amp; = \frac 1 3 \\   \Pr(A_g|B) &amp; =  \frac{\Pr(B | A_g) \Pr(A_g)}{\Pr(B)} &amp; =   \frac{0 \frac 1 3}{\frac 1 2} &amp; = 0 \\   \Pr(A_b|B) &amp; =  \frac{\Pr(B | A_b) \Pr(A_b)}{\Pr(B)} &amp; =   \frac{1 \frac 1 3}{\frac 1 2} &amp; = \frac 2 3 \end{matrix}" /></p>
<p>Note how this depends on the value of B. Let us suppose that if the prize is behind the red door, then the probability that the host will pick the green door is very <i>high</i>: 90% for instance. That is - the host will pick the green door unless he is forced not to.</p>
<p>The value of <b>B</b> then becomes 1/3 * 1 + 1/3 * 0 + 1/3 * 9/10 = 19/30.</p>
<p><img class='tex' src="../../../math/1/c/9/1c9d466c6ac39a714a8b2de0ab5dd6c6.png" alt="\begin{matrix}   \Pr(A_r|B) &amp; =  \frac{\Pr(B | A_r) \Pr(A_r)}{\Pr(B)} &amp; =   \frac{\frac 9 {10} \frac 1 3}{\frac {19} {30}} &amp; = \frac 9 {19} \\   \Pr(A_g|B) &amp; =  \frac{\Pr(B | A_g) \Pr(A_g)}{\Pr(B)} &amp; =   \frac{0 \frac 1 3}{\frac {19} {30}} &amp; = 0 \\   \Pr(A_b|B) &amp; =  \frac{\Pr(B | A_b) \Pr(A_b)}{\Pr(B)} &amp; =   \frac{1 \frac 1 3}{\frac {19} {30}} &amp; = \frac {10} {19} \end{matrix}" /></p>
<p>So in this situation, the host picking the green door tells us very little - he would probably have picked it anyway. <b>Pr(A<sub>b</sub>)</b> is only slightly better than 1/2.</p>
<p>Let us, by contrast, suppose that if the prize is behind the red door, then the probability that the host will pick the green door is very <i>low</i>: 10% for instance. That is - the host will almost never pick the green door unless he is forced to.</p>
<p>The value of <b>B</b> then becomes 1/3 * 1 + 1/3 * 0 + 1/3 * 1/10 = 11/30.</p>
<p><img class='tex' src="../../../math/2/7/0/2706954355cebf966eab890289f13f67.png" alt="\begin{matrix}   \Pr(A_r|B) &amp; =  \frac{\Pr(B | A_r) \Pr(A_r)}{\Pr(B)} &amp; =   \frac{\frac 1 {10} \frac 1 3}{\frac {11} {30}} &amp; = \frac 1 {11} \\   \Pr(A_g|B) &amp; =  \frac{\Pr(B | A_g) \Pr(A_g)}{\Pr(B)} &amp; =   \frac{0 \frac 1 3}{\frac {11} {30}} &amp; = 0 \\   \Pr(A_b|B) &amp; =  \frac{\Pr(B | A_b) \Pr(A_b)}{\Pr(B)} &amp; =   \frac{1 \frac 1 3}{\frac {11} {30}} &amp; = \frac {10} {11} \end{matrix}" /></p>
<p>In this situation, the fact that the host has chosen the green door tells us a great deal. The prize is almost certainly behind the blue door. If it were not then the host would very probably have picked it.</p>
<p><a name="Historical_remarks" id="Historical_remarks"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Historical remarks">edit</a>]</span> <span class="mw-headline">Historical remarks</span></h2>
<p>Bayes' theorem is named after the Reverend <a href="../../../t/h/o/Thomas_Bayes_3ee9.html" title="Thomas Bayes">Thomas Bayes</a> (<a href="../../../1/7/0/1702.html" title="1702">1702</a>–<a href="../../../1/7/6/1761.html" title="1761">1761</a>), who studied how to compute a distribution for the parameter of a <a href="../../../b/i/n/Binomial_distribution.html" title="Binomial distribution">binomial distribution</a> (to use modern terminology). His friend, <a href="../../../r/i/c/Richard_Price_5fd9.html" title="Richard Price">Richard Price</a>, edited and presented the work in <a href="../../../1/7/6/1763.html" title="1763">1763</a>, after Bayes' death, as <i>An Essay towards solving a Problem in the Doctrine of Chances</i>. <a href="../../../p/i/e/Pierre-Simon_Laplace_cc1f.html" title="Pierre-Simon Laplace">Pierre-Simon Laplace</a> replicated and extended these results in an essay of <a href="../../../1/7/7/1774.html" title="1774">1774</a>, apparently unaware of Bayes' work.</p>
<p>One of Bayes' results (Proposition 5) gives a simple description of <a href="../../../c/o/n/Conditional_probability.html" title="Conditional probability">conditional probability</a>, and shows that it can be expressed independently of the order in which things occur:</p>
<dl>
<dd><i>If there be two subsequent events, the probability of the second b/N and the probability of both together P/N, and it being first discovered that the second event has also happened, from hence I guess that the first event has also happened, the probability I am right</i> [i.e., the conditional probability of the first event being true given that the second has also happened] <i>is P/b.</i></dd>
</dl>
<p>Note that the expression says nothing about the <i>order</i> in which the events occurred; it measures correlation, not causation. His preliminary results, in particular Propositions 3, 4, and 5, imply the result now called Bayes' Theorem (as described above), but it does not appear that Bayes himself emphasized or focused on that result.</p>
<p>Bayes' main result (Proposition 9 in the essay) is the following: assuming a <a href="../../../u/n/i/Uniform_distribution.html" title="Uniform distribution">uniform distribution</a> for the <a href="../../../p/r/i/Prior_distribution.html" title="Prior distribution">prior distribution</a> of the <a href="../../../b/i/n/Binomial.html" title="Binomial">binomial</a> parameter <i>p</i>, the probability that <i>p</i> is between two values <i>a</i> and <i>b</i> is</p>
<dl>
<dd><img class='tex' src="../../../math/2/b/d/2bd02825f98ac442e1819e108ab10de5.png" alt="\frac {\int_a^b {n+m \choose m} p^m (1-p)^n\,dp}  {\int_0^1 {n+m \choose m} p^m (1-p)^n\,dp} \!" /></dd>
</dl>
<p>where <i>m</i> is the number of observed successes and <i>n</i> the number of observed failures.</p>
<p>What is "Bayesian" about Proposition 9 is that Bayes presented it as a probability for the parameter <i>p</i>. So, one can compute probability for an experimental outcome, but also for the parameter which governs it, and the same algebra is used to make inferences of either kind.</p>
<p>Bayes states his question in a way that might make the idea of assigning a probability distribution to a parameter palatable to a frequentist. He supposes that a billiard ball is thrown at random onto a billiard table, and that the probabilities <i>p</i> and <i>q</i> are the probabilities that subsequent billiard balls will fall above or below the first ball.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="../../../b/a/y/Bayesian_inference.html" title="Bayesian inference">Bayesian inference</a></li>
<li><a href="../../../b/a/y/Bayesian_spam_filtering.html" title="Bayesian spam filtering">Bayesian spam filtering</a></li>
<li><a href="../../../b/o/g/Bogofilter.html" title="Bogofilter">Bogofilter</a></li>
<li><a href="../../../c/o/n/Conjugate_prior.html" title="Conjugate prior">Conjugate prior</a></li>
<li><a href="../../../e/m/p/Empirical_Bayes_method_a47c.html" title="Empirical Bayes method">Empirical Bayes method</a></li>
<li><a href="../../../m/o/n/Monty_Hall_problem_4d49.html" title="Monty Hall problem">Monty Hall problem</a></li>
<li><a href="../../../o/c/c/Occam%27s_razor.html" title="Occam's razor">Occam's razor</a></li>
<li><a href="../../../p/r/o/Prosecutor%27s_fallacy.html" title="Prosecutor's fallacy">Prosecutor's fallacy</a></li>
<li><a href="../../../r/a/v/Raven_paradox.html" title="Raven paradox">Raven paradox</a></li>
<li><a href="../../../r/e/c/Recursive_Bayesian_estimation_0c29.html" title="Recursive Bayesian estimation">Recursive Bayesian estimation</a></li>
<li><a href="../../../r/e/v/Revising_opinions_in_statistics.html" title="Revising opinions in statistics">Revising opinions in statistics</a></li>
<li><a href="../../../s/e/q/Sequential_bayesian_filtering.html" title="Sequential bayesian filtering">Sequential bayesian filtering</a></li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ol class="references"></ol>
<p><a name="Versions_of_the_essay" id="Versions_of_the_essay"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Versions of the essay">edit</a>]</span> <span class="mw-headline">Versions of the essay</span></h3>
<ul>
<li>Thomas Bayes (1763), "An Essay towards solving a Problem in the Doctrine of Chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S.", <i><a href="../../../p/h/i/Philosophical_Transactions_072e.html" title="Philosophical Transactions">Philosophical Transactions</a>, Giving Some Account of the Present Undertakings, Studies and Labours of the Ingenious in Many Considerable Parts of the World</i> 53:370–418.</li>
<li>Thomas Bayes (1763/1958) "Studies in the History of Probability and Statistics: IX. Thomas Bayes' Essay Towards Solving a Problem in the Doctrine of Chances", <i><a href="../../../b/i/o/Biometrika.html" title="Biometrika">Biometrika</a></i> 45:296–315. <i>(Bayes' essay in modernized notation)</i></li>
<li>Thomas Bayes <a href="http://www.stat.ucla.edu/history/essay.pdf" class="external text" title="http://www.stat.ucla.edu/history/essay.pdf" rel="nofollow">"An essay towards solving a Problem in the Doctrine of Chances"</a>. <i>(Bayes' essay in the original notation)</i></li>
</ul>
<p><a name="Commentaries" id="Commentaries"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Commentaries">edit</a>]</span> <span class="mw-headline">Commentaries</span></h3>
<ul>
<li><a href="../../../g/e/o/George_Alfred_Barnard_e319.html" title="George Alfred Barnard">G. A. Barnard</a> (1958) "Studies in the History of Probability and Statistics: IX. Thomas Bayes' Essay Towards Solving a Problem in the Doctrine of Chances", <i>Biometrika</i> 45:293–295. <i>(biographical remarks)</i></li>
<li>Daniel Covarrubias. <a href="http://www.stat.rice.edu/~blairc/seminar/Files/danTalk.pdf" class="external text" title="http://www.stat.rice.edu/~blairc/seminar/Files/danTalk.pdf" rel="nofollow">"An Essay Towards Solving a Problem in the Doctrine of Chances"</a>. <i>(an outline and exposition of Bayes' essay)</i></li>
<li>Stephen M. Stigler (1982). "Thomas Bayes' Bayesian Inference," <i>Journal of the Royal Statistical Society</i>, Series A, 145:250–258. <i>(Stigler argues for a revised interpretation of the essay; recommended)</i></li>
<li><a href="../../../i/s/a/Isaac_Todhunter_f924.html" title="Isaac Todhunter">Isaac Todhunter</a> (1865). <i>A History of the Mathematical Theory of Probability from the time of Pascal to that of Laplace</i>, Macmillan. Reprinted 1949, 1956 by Chelsea and 2001 by Thoemmes.</li>
</ul>
<p><a name="Additional_material" id="Additional_material"></a></p>
<h3><span class="editsection">[<a href="../../../b/a/y/Bayes%27_theorem.html" title="Edit section: Additional material">edit</a>]</span> <span class="mw-headline">Additional material</span></h3>
<ul>
<li>Pierre-Simon Laplace (1774). "Mémoire sur la Probabilité des Causes par les Événements", <i>Savants Étranges</i> 6:621–656; also <i>Œuvres</i> 8:27–65.</li>
<li>Pierre-Simon Laplace (1774/1986). "Memoir on the Probability of the Causes of Events", <i>Statistical Science</i> 1(3):364–378.</li>
<li>Stephen M. Stigler (1986). "Laplace's 1774 memoir on inverse probability", <i>Statistical Science</i> 1(3):359–378.</li>
<li>Stephen M. Stigler (1983). "Who Discovered Bayes' Theorem?" <i>The American Statistician</i> 37(4):290–296.</li>
<li>Jeff Miller et al. <a href="http://members.aol.com/jeff570/mathword.html" class="external text" title="http://members.aol.com/jeff570/mathword.html" rel="nofollow">Earliest Known Uses of Some of the Words of Mathematics (B)</a>. (<i>very informative; recommended</i>)</li>
<li><a href="../../../a/t/h/Athanasios_Papoulis_31a6.html" title="Athanasios Papoulis">Athanasios Papoulis</a> (1984). <i>Probability, Random Variables, and Stochastic Processes</i>, second edition. New York: McGraw-Hill.</li>
<li>James Joyce (2003). <a href="http://plato.stanford.edu/entries/Bayes-theorem/" class="external text" title="http://plato.stanford.edu/entries/Bayes-theorem/" rel="nofollow">"Bayes' Theorem"</a>, <i><a href="../../../s/t/a/Stanford_Encyclopedia_of_Philosophy_2efe.html" title="Stanford Encyclopedia of Philosophy">Stanford Encyclopedia of Philosophy</a></i>.</li>
<li><a href="http://www.inference.phy.cam.ac.uk/mackay/itila/" class="external text" title="http://www.inference.phy.cam.ac.uk/mackay/itila/" rel="nofollow">The on-line textbook: Information Theory, Inference, and Learning Algorithms</a>, by <a href="../../../d/a/v/David_J.C._MacKay_3b78.html" title="David J.C. MacKay">David J.C. MacKay</a> provides an up to date overview of the use of Bayes' theorem in information theory and machine learning.</li>
<li><a href="http://plato.stanford.edu/entries/bayes-theorem/" class="external text" title="http://plato.stanford.edu/entries/bayes-theorem/" rel="nofollow">Stanford Encyclopedia of Philosophy: Bayes' Theorem</a> provides a comprehensive introduction to Bayes' theorem.</li>
<li>Eric W. Weisstein, <i><a href="http://mathworld.wolfram.com/BayesTheorem.html" class="external text" title="http://mathworld.wolfram.com/BayesTheorem.html" rel="nofollow">Bayes' Theorem</a></i> at <a href="../../../m/a/t/MathWorld_3918.html" title="MathWorld">MathWorld</a>.</li>
<li><i><a href="http://planetmath.org/encyclopedia/BayesTheorem.html" class="external text" title="http://planetmath.org/encyclopedia/BayesTheorem.html" rel="nofollow">Bayes' theorem</a></i> at <a href="../../../p/l/a/PlanetMath_a40d.html" title="PlanetMath">PlanetMath</a>.</li>
<li>Yudkowsky, Eliezer S. (2003), "<a href="http://yudkowsky.net/bayes/bayes.html" class="external text" title="http://yudkowsky.net/bayes/bayes.html" rel="nofollow">An Intuitive Explanation of Bayesian Reasoning</a>"</li>
</ul>

<!-- 
Pre-expand include size: 1113 bytes
Post-expand include size: 196 bytes
Template argument size: 69 bytes
Maximum: 2048000 bytes
-->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org../../../b/a/y/Bayes%27_theorem.html">http://en.wikipedia.org../../../b/a/y/Bayes%27_theorem.html</a>"</div>
	    <div id="catlinks"><p class='catlinks'><a href="../../../c/a/t/Special%7ECategories_101d.html" title="Special:Categories">Categories</a>: <span dir='ltr'><a href="../../../p/r/o/Category%7EProbability_theory_ef79.html" title="Category:Probability theory">Probability theory</a></span> | <span dir='ltr'><a href="../../../m/a/t/Category%7EMathematical_theorems_3848.html" title="Category:Mathematical theorems">Mathematical theorems</a></span></p></div>	    <!-- end content -->
	    <div class="visualClear"></div>
	  </div>
	</div>
      </div>
      <div id="column-one">
	<div id="p-cactions" class="portlet">
	  <h5>Views</h5>
	  <ul>
	    <li id="ca-nstab-main"
	       class="selected"	       ><a href="../../../b/a/y/Bayes%27_theorem.html">Article</a></li><li id="ca-talk"
	       	       ><a href="../../../b/a/y/Talk%7EBayes%27_theorem_a302.html">Discussion</a></li><li id="ca-current"
	       	       ><a href="http://en.wikipedia.org/wiki/Bayes%27_theorem">Current revision</a></li>	  </ul>
	</div>
	<div class="portlet" id="p-logo">
	  <a style="background-image: url(../../../images/wiki-en.png);"
	    href="../../../index.html"
	    title="Main Page"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
		<div class='portlet' id='p-navigation'>
	  <h5>Navigation</h5>
	  <div class='pBody'>
	    <ul>
	    	      <li id="n-Main-page"><a href="../../../index.html">Main page</a></li>
	     	      <li id="n-Contents"><a href="../../../c/o/n/Wikipedia%7EContents_3181.html">Contents</a></li>
	     	      <li id="n-Featured-content"><a href="../../../f/e/a/Wikipedia%7EFeatured_content_24ba.html">Featured content</a></li>
	     	      <li id="n-currentevents"><a href="../../../c/u/r/Portal%7ECurrent_events_bb60.html">Current events</a></li>
	     	    </ul>
	  </div>
	</div>
		<div class='portlet' id='p-interaction'>
	  <h5>interaction</h5>
	  <div class='pBody'>
	    <ul>
	    	      <li id="n-About-Wikipedia"><a href="../../../a/b/o/Wikipedia%7EAbout_8d82.html">About Wikipedia</a></li>
	     	      <li id="n-portal"><a href="../../../c/o/m/Wikipedia%7ECommunity_Portal_6a3c.html">Community portal</a></li>
	     	      <li id="n-contact"><a href="../../../c/o/n/Wikipedia%7EContact_us_afd6.html">Contact us</a></li>
	     	      <li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Fundraising">Make a donation</a></li>
	     	      <li id="n-help"><a href="../../../c/o/n/Help%7EContents_22de.html">Help</a></li>
	     	    </ul>
	  </div>
	</div>
		<div id="p-search" class="portlet">
	  <h5><label for="searchInput">Search</label></h5>
	  <div id="searchBody" class="pBody">
	    <form action="javascript:goToStatic(3)" id="searchform"><div>
	      <input id="searchInput" name="search" type="text"
	        accesskey="f" value="" />
	      <input type='submit' name="go" class="searchButton" id="searchGoButton"
	        value="Go" />
	    </div></form>
	  </div>
	</div>
	<div id="p-lang" class="portlet">
	  <h5>In other languages</h5>
	  <div class="pBody">
	    <ul>
	      	      <li>
	      <a href="../../../../ar/%D9%85/%D8%A8/%D8%B1/%D9%85%D8%A8%D8%B1%D9%87%D9%86%D8%A9_%D8%A8%D8%A7%D9%8A%D8%B2.html">العربية</a>
	      </li>
	      	      <li>
	      <a href="../../../../ca/t/e/o/Teorema_de_Bayes_9d96.html">Català</a>
	      </li>
	      	      <li>
	      <a href="../../../../de/b/a/y/Bayestheorem.html">Deutsch</a>
	      </li>
	      	      <li>
	      <a href="../../../../es/t/e/o/Teorema_de_Bayes_9d96.html">Español</a>
	      </li>
	      	      <li>
	      <a href="../../../../fr/t/h/%C3%A9/Th%C3%A9or%C3%A8me_de_Bayes_e468.html">Français</a>
	      </li>
	      	      <li>
	      <a href="../../../../it/t/e/o/Teorema_di_Bayes_f931.html">Italiano</a>
	      </li>
	      	      <li>
	      <a href="../../../../he/%D7%97/%D7%95/%D7%A7/%D7%97%D7%95%D7%A7_%D7%91%D7%99%D7%99%D7%A1.html">עברית</a>
	      </li>
	      	      <li>
	      <a href="../../../../nl/t/h/e/Theorema_van_Bayes_2fc4.html">Nederlands</a>
	      </li>
	      	      <li>
	      <a href="../../../../ja/%E3%83%99/%E3%82%A4/%E3%82%BA/%E3%83%99%E3%82%A4%E3%82%BA%E3%81%AE%E5%AE%9A%E7%90%86.html">日本語</a>
	      </li>
	      	      <li>
	      <a href="../../../../pl/t/w/i/Twierdzenie_Bayesa_65e6.html">Polski</a>
	      </li>
	      	      <li>
	      <a href="../../../../pt/t/e/o/Teorema_de_Bayes_9d96.html">Português</a>
	      </li>
	      	      <li>
	      <a href="../../../../ro/t/e/o/Teorema_lui_Bayes_e6e4.html">Română</a>
	      </li>
	      	      <li>
	      <a href="../../../../ru/%D1%82/%D0%B5/%D0%BE/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%B0_b767.html">Русский</a>
	      </li>
	      	      <li>
	      <a href="../../../../sr/%D0%B1/%D0%B0/%D1%98/%D0%91%D0%B0%D1%98%D0%B5%D1%81%D0%BE%D0%B2%D0%B0_%D1%82%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0.html">Српски / Srpski</a>
	      </li>
	      	      <li>
	      <a href="../../../../su/t/%C3%A9/o/T%C3%A9or%C3%A9ma_Bayes_ae29.html">Basa Sunda</a>
	      </li>
	      	      <li>
	      <a href="../../../../fi/b/a/y/Bayesin_teoreema.html">Suomi</a>
	      </li>
	      	      <li>
	      <a href="../../../../sv/b/a/y/Bayes_sats.html">Svenska</a>
	      </li>
	      	      <li>
	      <a href="../../../../vi/%C4%91/%E1%BB%8B/n/%C4%90%E1%BB%8Bnh_l%C3%BD_Bayes_e2ce.html">Tiếng Việt</a>
	      </li>
	      	      <li>
	      <a href="../../../../zh/%E8%B4%9D/%E5%8F%B6/%E6%96%AF/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86.html">中文</a>
	      </li>
	      	    </ul>
	  </div>
	</div>
	      </div><!-- end of the left (by default at least) column -->
      <div class="visualClear"></div>
      <div id="footer">
    <div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="../../../skins/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>	<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="../../../images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>	<ul id="f-list">
	  	  	  <li id="f-credits">This page was last modified 02:30, 5 April 2007 by Anonymous user(s) of Wikipedia. Based on work by Wikipedia user(s) <a href="../../../k/z/o/User%7EKzollman_a660.html" title="User:Kzollman">Kzollman</a>, <a href="../../../t/s/c/User%7ETsca.bot_2336.html" title="User:Tsca.bot">Tsca.bot</a>, <a href="../../../k/u/p/User%7EKupirijo_34da.html" title="User:Kupirijo">Kupirijo</a>, Forwardmeasure, <a href="../../../k/r/i/User%7EKripkenstein_3260.html" title="User:Kripkenstein">Kripkenstein</a>, <a href="../../../h/e/n/User%7EHenrygb_9b92.html" title="User:Henrygb">Henrygb</a>, Makana, <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Billjefferys</a>, <a href="../../../j/m/o/User%7EJmorgan_e975.html" title="User:Jmorgan">Jmorgan</a>, <a href="../../../j/m/a/User%7EJmath666_d2d8.html" title="User:Jmath666">Jmath666</a>, Karinhunter, <a href="../../../s/t/e/User%7EStephen_B_Streater_f6ab.html" title="User:Stephen B Streater">Stephen B Streater</a>, <a href="../../../c/h/u/User%7EChuckHG_e510.html" title="User:ChuckHG">ChuckHG</a>, <a href="../../../v/s/b/User%7EVsb_dd9d.html" title="User:Vsb">Vsb</a>, Blocter, <a href="../../../g/i/m/User%7EGimboid_01b6.html" title="User:Gimboid">Gimboid</a>, Weakishspeller, <a href="../../../t/h/i/User%7EThijs%21bot_bdf4.html" title="User:Thijs!bot">Thijs!bot</a>, <a href="../../../j/o/d/User%7EJodi.a.schneider_5c40.html" title="User:Jodi.a.schneider">Jodi.a.schneider</a>, <a href="../../../p/a/u/User%7EPaul_Murray_43d8.html" title="User:Paul Murray">Paul Murray</a>, <a href="../../../p/3/d/User%7EP3d0_7b66.html" title="User:P3d0">P3d0</a>, Pianoroy, <a href="../../../j/u/s/User%7EJustinHagstrom_1024.html" title="User:JustinHagstrom">JustinHagstrom</a>, <a href="../../../d/c/l/User%7EDcljr_a662.html" title="User:Dcljr">Dcljr</a>, <a href="../../../a/n/t/User%7EAntiVandalBot_aa84.html" title="User:AntiVandalBot">AntiVandalBot</a>, Plynn9, <a href="../../../r/i/t/User%7ERitchy_3c4d.html" title="User:Ritchy">Ritchy</a>, Primarscources, <a href="../../../t/i/m/User%7ETimwi_eb8f.html" title="User:Timwi">Timwi</a>, <a href="../../../m/a/r/User%7EMarkSweep_b58d.html" title="User:MarkSweep">MarkSweep</a>, Jordanolsommer, <a href="../../../f/l/a/User%7EFlaBot_747f.html" title="User:FlaBot">FlaBot</a>, <a href="../../../w/a/f/User%7EWafulz_fbcf.html" title="User:Wafulz">Wafulz</a>, <a href="../../../m/i/c/User%7EMichael_Hardy_e932.html" title="User:Michael Hardy">Michael Hardy</a>, <a href="../../../m/e/r/User%7EMER-C_4926.html" title="User:MER-C">MER-C</a>, <a href="../../../t/y/d/User%7ETydeNet_0b50.html" title="User:TydeNet">TydeNet</a>, <a href="../../../j/f/m/User%7EJfmarchini_f9dc.html" title="User:Jfmarchini">Jfmarchini</a>, Salgueiro, <a href="../../../c/h/i/User%7EChinju_e6fd.html" title="User:Chinju">Chinju</a>, Daniel il, Karen Huyser, <a href="../../../z/v/e/User%7EZven_18ae.html" title="User:Zven">Zven</a>, <a href="../../../l/i/n/User%7ELindsay658_d7de.html" title="User:Lindsay658">Lindsay658</a>, <a href="../../../h/a/n/User%7EHannes_Eder_3b19.html" title="User:Hannes Eder">Hannes Eder</a>, <a href="../../../d/h/n/User%7EDHN-bot_8c4a.html" title="User:DHN-bot">DHN-bot</a>, <a href="../../../t/o/m/User%7ETomyumgoong_d4a8.html" title="User:Tomyumgoong">Tomyumgoong</a>, <a href="../../../s/h/o/User%7EShotgunlee_6c20.html" title="User:Shotgunlee">Shotgunlee</a>, <a href="../../../t/o/m/User%7EToms2866_7e19.html" title="User:Toms2866">Toms2866</a>, <a href="../../../a/n/t/User%7EAntaeus_Feldspar_9a66.html" title="User:Antaeus Feldspar">Antaeus Feldspar</a>, Amosfolarin, Ceran, Weather Man, <a href="../../../u/r/h/User%7EUrhixidur_9dbb.html" title="User:Urhixidur">Urhixidur</a>, <a href="../../../v/e/r/User%7EVerne_Equinox_8d35.html" title="User:Verne Equinox">Verne Equinox</a>, <a href="../../../i/v/a/User%7EIvan_F._Villanueva_B._7494.html" title="User:Ivan F. Villanueva B.">Ivan F. Villanueva B.</a>, <a href="../../../b/u/t/User%7EButros_f184.html" title="User:Butros">Butros</a>, <a href="../../../e/l/_/User%7EEl_C_6d95.html" title="User:El C">El C</a>, <a href="../../../c/h/a/User%7ECharles_Matthews_cceb.html" title="User:Charles Matthews">Charles Matthews</a>, <a href="../../../m/a/r/User%7EMarlow4_c20f.html" title="User:Marlow4">Marlow4</a>, <a href="../../../t/e/r/User%7ETerra_Green_fe2e.html" title="User:Terra Green">Terra Green</a>, <a href="../../../y/u/r/User%7EYurikBot_b393.html" title="User:YurikBot">YurikBot</a>, <a href="../../../m/a/t/User%7EMathbot_604f.html" title="User:Mathbot">Mathbot</a>, <a href="../../../m/e/t/User%7EMetacomet_63d1.html" title="User:Metacomet">Metacomet</a>, <a href="../../../w/i/l/User%7EWile_E._Heresiarch_9879.html" title="User:Wile E. Heresiarch">Wile E. Heresiarch</a>, <a href="../../../j/o/g/User%7EJogloran_a21c.html" title="User:Jogloran">Jogloran</a>, <a href="../../../b/e/t/User%7EBeteNoir_e731.html" title="User:BeteNoir">BeteNoir</a>, <a href="../../../g/e/n/User%7EGeni_1ef9.html" title="User:Geni">Geni</a>, Marquez, <a href="../../../b/j/c/User%7EBjcairns_871c.html" title="User:Bjcairns">Bjcairns</a>, <a href="../../../s/i/m/User%7ESimoes_d467.html" title="User:Simoes">Simoes</a>, <a href="../../../n/a/r/User%7ENarxysus_572b.html" title="User:Narxysus">Narxysus</a>, <a href="../../../o/l/e/User%7EOleg_Alexandrov_8bb5.html" title="User:Oleg Alexandrov">Oleg Alexandrov</a>, <a href="../../../d/r/a/User%7EDrallim_11c2.html" title="User:Drallim">Drallim</a>, <a href="../../../s/t/o/User%7EStoni_ea39.html" title="User:Stoni">Stoni</a>, <a href="../../../s/o/n/User%7ESonett72_c1f8.html" title="User:Sonett72">Sonett72</a>, <a href="../../../s/t/e/User%7EStevertigo_b55d.html" title="User:Stevertigo">Stevertigo</a>, <a href="../../../e/d/h/User%7EEdH_fee0.html" title="User:EdH">EdH</a>, <a href="../../../d/y/s/User%7EDyslexicEditor_aec8.html" title="User:DyslexicEditor">DyslexicEditor</a>, <a href="../../../c/h/a/User%7EChalst_cb81.html" title="User:Chalst">Chalst</a>, <a href="../../../d/u/n/User%7EDuncharris_e9cb.html" title="User:Duncharris">Duncharris</a>, <a href="../../../s/a/m/User%7ESam_Hocevar_dccf.html" title="User:Sam Hocevar">Sam Hocevar</a>, <a href="../../../t/i/e/User%7ETietew_cf0c.html" title="User:Tietew">Tietew</a>, <a href="../../../m/a/x/User%7EMaximaximax_5f36.html" title="User:Maximaximax">Maximaximax</a>, <a href="../../../b/k/e/User%7EBkell_6f8f.html" title="User:Bkell">Bkell</a>, <a href="../../../g/u/a/User%7EGuanabot_b902.html" title="User:Guanabot">Guanabot</a>, Josh Griffith, <a href="../../../h/i/k/User%7EHike395_91ce.html" title="User:Hike395">Hike395</a>, <a href="../../../l/u/p/User%7ELupin_68a5.html" title="User:Lupin">Lupin</a>, <a href="../../../p/c/a/User%7EPcarbonn_cbe1.html" title="User:Pcarbonn">Pcarbonn</a>, <a href="../../../l/a/n/User%7ELancevortex_a6a9.html" title="User:Lancevortex">Lancevortex</a>, <a href="../../../m/i/g/User%7EMiguel_74e3.html" title="User:Miguel">Miguel</a>, <a href="../../../m/a/r/User%7EMaroux_67b6.html" title="User:Maroux">Maroux</a>, <a href="../../../r/a/n/User%7ERandywombat_6d0e.html" title="User:Randywombat">Randywombat</a>, <a href="../../../t/a/k/User%7ETakuyaMurata_085a.html" title="User:TakuyaMurata">TakuyaMurata</a>, <a href="../../../s/n/o/User%7ESnobot_2eb5.html" title="User:Snobot">Snobot</a>, <a href="../../../t/h/e/User%7EThe_Anome_61a9.html" title="User:The Anome">The Anome</a>, <a href="../../../d/a/v/User%7EDavidLevinson_6300.html" title="User:DavidLevinson">DavidLevinson</a>, <a href="../../../s/p/e/User%7ESpellbinder_2bbc.html" title="User:Spellbinder">Spellbinder</a>, <a href="../../../k/a/r/User%7EKarada_595b.html" title="User:Karada">Karada</a>, <a href="../../../a/l/f/User%7EAlfio_ae11.html" title="User:Alfio">Alfio</a>, <a href="../../../c/o/l/User%7EColin_Marquardt_1f2f.html" title="User:Colin Marquardt">Colin Marquardt</a>, <a href="../../../a/p/_/User%7EAp_24b6.html" title="User:Ap">Ap</a>, <a href="../../../h/e/r/User%7EHeron_606a.html" title="User:Heron">Heron</a>, <a href="../../../s/n/o/User%7ESnoyes_3ec6.html" title="User:Snoyes">Snoyes</a>, <a href="../../../t/a/w/User%7ETaw_8bc0.html" title="User:Taw">Taw</a> and <a href="../../../a/x/e/User%7EAxelBoldt_4306.html" title="User:AxelBoldt">AxelBoldt</a>.</li>	  <li id="f-copyright">All text is available under the terms of the <a class='internal' href="../../../t/e/x/Wikipedia%7EText_of_the_GNU_Free_Documentation_License_702a.html" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="../../../c/o/p/Wikipedia%7ECopyrights_92c4.html" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc</a>., a US-registered <a class='internal' href="../../../5/0/1/501%28c%29.html#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="../../../n/o/n/Non-profit_organization.html" title="Non-profit organization">nonprofit</a> <a href="../../../c/h/a/Charitable_organization.html" title="Charitable organization">charity</a>.<br /></li>	  <li id="f-about"><a href="../../../a/b/o/Wikipedia%7EAbout_8d82.html" title="Wikipedia:About">About Wikipedia</a></li>	  <li id="f-disclaimer"><a href="../../../g/e/n/Wikipedia%7EGeneral_disclaimer_3e44.html" title="Wikipedia:General disclaimer">Disclaimers</a></li>	  	</ul>
      </div>
    </div>
  </body>
</html>
