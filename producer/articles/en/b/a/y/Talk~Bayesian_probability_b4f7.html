<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    		<meta name="keywords" content="Talk:Bayesian probability,Admissible decision rules,Bayes Theorem,Bayesian probability,Beta distribution,Conjugate prior,Cox's theorem,Credence,Dempster-Shafer theory,Dirac delta function,Edwin Jaynes" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (English)" />
		<link rel="copyright" href="../../../COPYING.html" />
    <title>Talk:Bayesian probability - Wikipedia, the free encyclopedia</title>
    <style type="text/css">/*<![CDATA[*/ @import "../../../skins/htmldump/main.css"; /*]]>*/</style>
    <link rel="stylesheet" type="text/css" media="print" href="../../../skins/common/commonPrint.css" />
    <!--[if lt IE 5.5000]><style type="text/css">@import "../../../skins/monobook/IE50Fixes.css";</style><![endif]-->
    <!--[if IE 5.5000]><style type="text/css">@import "../../../skins/monobook/IE55Fixes.css";</style><![endif]-->
    <!--[if IE 6]><style type="text/css">@import "../../../skins/monobook/IE60Fixes.css";</style><![endif]-->
    <!--[if IE]><script type="text/javascript" src="../../../skins/common/IEFixes.js"></script>
    <meta http-equiv="imagetoolbar" content="no" /><![endif]-->
    <script type="text/javascript" src="../../../skins/common/wikibits.js"></script>
    <script type="text/javascript" src="../../../skins/htmldump/md5.js"></script>
    <script type="text/javascript" src="../../../skins/htmldump/utf8.js"></script>
    <script type="text/javascript" src="../../../skins/htmldump/lookup.js"></script>
    <script type="text/javascript" src="../../../raw/gen.js"></script>        <style type="text/css">/*<![CDATA[*/
@import "../../../raw/MediaWiki%7ECommon.css";
@import "../../../raw/MediaWiki%7EMonobook.css";
@import "../../../raw/gen.css";
/*]]>*/</style>          </head>
  <body
    class="ns-1">
    <div id="globalWrapper">
      <div id="column-content">
	<div id="content">
	  <a name="top" id="contentTop"></a>
	        <h1 class="firstHeading">Talk:Bayesian probability</h1>
	  <div id="bodyContent">
	    <h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
	    <div id="contentSub"></div>
	    	    <div class="usermessage">You have <a href="../../../1/2/7/User_talk%7E127.0.0.1.html" title="User talk:127.0.0.1">new messages</a> (<a href="../../../1/2/7/User_talk%7E127.0.0.1.html" title="User talk:127.0.0.1">last change</a>).</div>	    <!-- start content -->
	    <div class="messagebox cleanup metadata plainlinks">
<table style="width:100%;background:none">
<tr>
<td width="60px"><a href="../../../i/n/f/Image%7EInformation_icon.svg_dac9.html" class="image" title=""><img src="../../../upload/thumb/3/35/Information_icon.svg/40px-Information_icon.svg.png" alt="" width="40" height="40" longdesc="../../../i/n/f/Image%7EInformation_icon.svg_dac9.html" /></a></td>
<td><b>This article may be too technical for a general audience.</b><br />
Please help improve this article by providing more context and better explanations of technical details to <a href="../../../m/a/k/Wikipedia%7EMake_technical_articles_accessible_9f90.html" title="Wikipedia:Make technical articles accessible">make it more accessible</a>, without removing technical details. See below for more information.</td>
</tr>
</table>
</div>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Reference_Classes"><span class="tocnumber">1</span> <span class="toctext">Reference Classes</span></a></li>
<li class="toclevel-1"><a href="#Types_of_probability:_Bayesian.2C_subjective.2C_personal.2C_epistemic"><span class="tocnumber">2</span> <span class="toctext">Types of probability: Bayesian, subjective, personal, epistemic</span></a></li>
<li class="toclevel-1"><a href="#Crow_Paradox"><span class="tocnumber">3</span> <span class="toctext">Crow Paradox</span></a></li>
<li class="toclevel-1"><a href="#Observation_and_question"><span class="tocnumber">4</span> <span class="toctext">Observation and question</span></a></li>
<li class="toclevel-1"><a href="#problem_of__information"><span class="tocnumber">5</span> <span class="toctext">problem of information</span></a></li>
<li class="toclevel-1"><a href="#Disputed:_Paradoxes_in_Bayesian_inference"><span class="tocnumber">6</span> <span class="toctext">Disputed: Paradoxes in Bayesian inference</span></a></li>
<li class="toclevel-1"><a href="#Another_dispute:_What_Laplace_meant"><span class="tocnumber">7</span> <span class="toctext">Another dispute: What Laplace meant</span></a></li>
<li class="toclevel-1"><a href="#Need_a_better_example_of_frequentist_probability"><span class="tocnumber">8</span> <span class="toctext">Need a better example of frequentist probability</span></a></li>
<li class="toclevel-1"><a href="#Beta_Prior_Distribution_at_Bayesian_data_analysis_example"><span class="tocnumber">9</span> <span class="toctext">Beta Prior Distribution at Bayesian data analysis example</span></a></li>
<li class="toclevel-1"><a href="#A_good_overview_for_.22dummies.22_would_be_nice."><span class="tocnumber">10</span> <span class="toctext">A good overview for "dummies" would be nice.</span></a></li>
<li class="toclevel-1"><a href="#Bogus_Portrait.3F"><span class="tocnumber">11</span> <span class="toctext">Bogus Portrait?</span></a></li>
<li class="toclevel-1"><a href="#Bayesian_and_frequentist_probability"><span class="tocnumber">12</span> <span class="toctext">Bayesian and frequentist probability</span></a></li>
<li class="toclevel-1"><a href="#Credence:_is_it_a_synonym_for_subjective_probability"><span class="tocnumber">13</span> <span class="toctext">Credence: is it a synonym for subjective probability</span></a></li>
<li class="toclevel-1"><a href="#subject_matter"><span class="tocnumber">14</span> <span class="toctext">subject matter</span></a></li>
</ul>
</td>
</tr>
</table>
<p><script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script><a name="Reference_Classes" id="Reference_Classes"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Reference Classes">edit</a>]</span> <span class="mw-headline">Reference Classes</span></h2>
<p>It's true that the remarks on the absence of reference classes are confusing, mostly because it's incorrect (now fixed in the controversy section at least). There must always be a reference class for any application of probability theory under anybody's interpretation. The basic point is that uncertainty is all expressed probabilistically. Whether the uncertainty comes from known empirical sampling variation in repeated trials, or just brute subjective states, is irrelevant. (Viz)</p>
<p>---</p>
<p>Hey authors, I'm sure the maths in this article is great, but as a non mathamatician, I can't get past the second paragraph in this article without being completely confused</p>
<p>"The Bayesian interpretation of probability allows probabilities to be assigned to all propositions (or, in some formulations, to the events signified by those propositions) independently of any reference class within which purported facts can be thought to have a relative frequency. "</p>
<p>Reference class? What is that? There is no link to what that means... Just an idea: a bit of a rewrite, so that non-mathematicians (or logical philosophers) can understand this article would be great.--<a href="../../../b/i/l/User%7EBilz0r_6726.html" title="User:Bilz0r">Bilz0r</a> 21:03, 28 March 2006 (UTC)</p>
<p>---</p>
<p><a name="Types_of_probability:_Bayesian.2C_subjective.2C_personal.2C_epistemic" id="Types_of_probability:_Bayesian.2C_subjective.2C_personal.2C_epistemic"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Types of probability: Bayesian, subjective, personal, epistemic">edit</a>]</span> <span class="mw-headline">Types of probability: Bayesian, subjective, personal, epistemic</span></h2>
<p>"Bayesian probability is also known as subjective probability, personal probability, or epistemic probability." Is this true? I'm no expert on this, but I thought subjective probability was more or less the "feel" for how probable something was, that personal probability was a very close synonym of subjective probability, and epistemic probability was, well, something different yet: the degree to which a person's belief is probably true given the total evidence that the person has. Again, I could easily be wrong, but wouldn't it be an at least somewhat controversial theory about subjective, personal, and epistemic probability to say that they are each reducible to Bayesian probability? --<a href="../../../l/m/s/LMS_66a8.html" title="LMS">LMS</a></p>
<dl>
<dd>The way you describe it, they all seem like the same thing. <a href="../../../c/o/x/Cox%27s_theorem.html" title="Cox's theorem">Cox's theorem</a> suggests that either they must be the same thing or that one or more of the theories contains inconsistencies or violates "common sense".</dd>
</dl>
<dl>
<dd>Bayesian probability are subjective in the sense that two different people can assign different probabilities to the same event because they each have different information available to them. Probabilities are subjective because they depend on what you know. In the frequentist world view probabilities are properties of the system, and don't vary with the observer. Jaynes talked about this a lot.-- [[User:Gavin Crooks|]</dd>
</dl>
<p>I definitely agree with Larry (i.e. "LMS") on this one. The fact that different probabilities may be assigned given different information certainly is not enough to mean they're subjective. <a href="../../../m/i/c/User%7EMichael_Hardy_e932.html" title="User:Michael Hardy">Michael Hardy</a> 30 June 2005 23:50 (UTC)</p>
<hr />
<p>They are not the same thing. The <i>mathematics</i> involved is the same thing according to what Richard Cox is saying, but what the mathematics is applied to is the difference. Mathematicians like to think probability theory is only about mathematics and nothing else, but that's just narrowness of mathematicians. (Shameless plug: See my paper on pages 243-292 of the August 2002 issue of <i>Advances in Applied Mathematics</i>. I assert there that I think Cox's assumptions are too strong, although I don't really say why. I <i>do</i> say what I would replace them with.) -- Mike Hardy</p>
<p>I agree with what Larry Sanger said above. The article was wrong to say the following (which I will alter once I've figured out what to replace it with): "The term 'subjective probability' arises since people with different 'prior information' could apply the theory correctly and obtain different probabilities for any particular statement. However given a particular set of prior information, the theory will always lead to the same conclusions." That people with the SAME prior information could assign DIFFERENT probabilities is what subjectivity suggests; if people with the same prior information were always led to the same epistemic probability assignments when they correctly applied the theory, then those would be epistemically OBJECTIVE assessments of probability. They would <i>not</i> be "objective" in the sense in which that word unfortunately gets used most often; i.e., the probabilities would not be "in the object"; they would not be relative frequencies of successes in independent trials, nor proportions of populations, etc. I distinguish between "logical" epistemic probabilities, which are epistemically objectively assigned (and nobody has any idea how to do that except in very simple cases) and "subjective" epistemic probabilities, which measure how sure someone is of something --- the "feel", as Larry puts it. I have been known to say in public that the words "random" and "event" should be banished from the theory of probability, but we're getting into gigantic Augean stables when we say that. <a href="../../../m/i/c/User%7EMichael_Hardy_e932.html" title="User:Michael Hardy">Michael Hardy</a> 22:27 Jan 18, 2003 (UTC)</p>
<dl>
<dd>It's hard to overstate the difficulties in understanding the foundations of probability. A quick web search turns up <a href="http://cepa.newschool.edu/het/essays/uncert/subjective.htm" class="external autonumber" title="http://cepa.newschool.edu/het/essays/uncert/subjective.htm" rel="nofollow">[1]</a> which mentions several schools of thought in a few paragraphs, inevitably including several different schools within the "subjective probability" camp! User:(</dd>
</dl>
<hr />
<p>It's not strictly correct to say that no one knows how to assign logical probabilities in non-trivial cases. For example, the empirical fact of <a href="http://mathworld.wolfram.com/BenfordsLaw.html" class="external text" title="http://mathworld.wolfram.com/BenfordsLaw.html" rel="nofollow">Benford's Law</a> can be logically explained using scale invariance, which is a special case of <a href="http://bayes.wustl.edu/etj/etj.html" class="external text" title="http://bayes.wustl.edu/etj/etj.html" rel="nofollow">E.T. Jaynes</a>'s Principle of transformation groups. Another non-trivial case which can solved with this principle is <a href="http://mathworld.wolfram.com/BertrandsProblem.html" class="external text" title="http://mathworld.wolfram.com/BertrandsProblem.html" rel="nofollow">Bertrand's Problem</a>. Jaynes's articles are available <a href="http://bayes.wustl.edu/etj/node1.html" class="external text" title="http://bayes.wustl.edu/etj/node1.html" rel="nofollow">here</a>. <a href="../../../c/y/a/User%7ECyan_b70d.html" title="User:Cyan">Cyan</a> 22:42 Mar 27, 2003 (UTC)</p>
<p>Jaynes' arguments are very interesting, but I don't think they have yet reached the stage of proof-beyond-all-reasonable-doubt, and there's a lot of work to be done before many statisticians can reliably apply them in typical applied statistics problems. <a href="../../../m/i/c/User%7EMichael_Hardy_e932.html" title="User:Michael Hardy">Michael Hardy</a> 00:31 Mar 28, 2003 (UTC)</p>
<p>But you must admit that while Jayes' position is not complete, it has wider scope and greater consistancy than the frequentist approach (muddle of ad-hoc methods)? For me, Jaynes' recent book makes this case, and does it by focusing on comparison of results (i've added an ext-link to it on his page). (I fear that you may be interpreted as implying 'even the best cars can get stuck in the mud - so you should always walk...'. While researchers rightly focus on what is left to be added, someone reading an encyclopedia is looking to learn what <i>is</i>) 193.116.20.220 16:58 16 May 2003 (UTC)</p>
<hr />
<dl>
<dd><i>It is also at times an attempt to describe the scientific method of starting with an initial set of beliefs about the relative plausiblity of various hypotheses, collecting new information (for example by conducting an experiment), and adjusting the original set of beliefs in the light of the new information to produce a more refined set of beliefs on the plausibility of the different hypotheses.</i></dd>
</dl>
<p>This sentence and the paragraph "Applications of Bayesian Probability" should be removed or moved to <a href="../../../b/a/y/Bayes_Theorem_60f9.html" title="Bayes Theorem">Bayes Theorem</a>. In order to avoid confusion it is crucial to distinguish the philosophical interpretation of probability from the mathematical formula developed by Bayes. These are not the same, they are often misunderstood, and the current version of the article makes it easy to get it wrong. Bayes' Theorem is a mathematical formula whos truth cannot be reasonably disputed. Bayes probability is the interpretation of mathematical construct (probability) and there was significant dispute in the past. I suggest we discuss the philosophy and the historical dispute in this article and the math with its applications in <a href="../../../b/a/y/Bayes_Theorem_60f9.html" title="Bayes Theorem">Bayes Theorem</a>. 134.155.53.23 14:28, 23 Dec 2003 (UTC)</p>
<p><a name="Crow_Paradox" id="Crow_Paradox"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Crow Paradox">edit</a>]</span> <span class="mw-headline">Crow Paradox</span></h2>
<p>Isn't this related to the All Crows are Black Paradox?</p>
<dl>
<dd><a href="../../../h/e/m/Hempel%27s_paradox.html" title="Hempel's paradox">Hempel's paradox</a> says that "All Crows are Black" (uncontroversial) logically implies "All not-Crows are not-Black" (manifestly untrue). What's your point? 217.42.117.73 14:46, 6 Mar 2005 (UTC)
<dl>
<dd>No, it's equivalent to "all non-black things are not crows". <a href="../../../b/a/n/User%7EBanno_733d.html" title="User:Banno">Banno</a> 19:44, Mar 6, 2005 (UTC)</dd>
</dl>
</dd>
<dd>Could it be that the paradox intended here is that a brown cow counterintuitively becomes a confirming instance of the hypothesis that all crows are black. <a href="../../../p/j/t/User%7EPJTraill_4845.html" title="User:PJTraill">PJTraill</a> 00:08, 6 November 2006 (UTC)</dd>
</dl>
<p><a name="Observation_and_question" id="Observation_and_question"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Observation and question">edit</a>]</span> <span class="mw-headline">Observation and question</span></h2>
<p>I don't know how or if this can be incorprated, but it's been my experience from comparison of frequentist multiple-range tests (e.g., Ryan-Einot-Gabriel-Welsch) with a Bayesian test (e.g., Waller-Duncan) that the former are more subject to perturbation by the overall distribution of the dataset. Specifically, if one mean is very much greater magnitude than all the other means, the frequentist test will assign the extreme mean to one group while assigning all other means to a second group, no matter how much difference there may be among the remaining means and no matter how tightly each treatment's results group around their respective means. The Bayesian test, on the other hand, does not do this. While none of us poor molecular biologists in our lab have our heads around the math, the Bayesian outcome "smells better" given our overall understanding of the specific system, developed over multiple experiments. Since we're cautious, we just report both analyses in our papers. <a href="../../../d/o/g/User%7EDogface_68d5.html" title="User:Dogface">Dogface</a> 04:07, 13 Oct 2004 (UTC)</p>
<p><a name="problem_of__information" id="problem_of__information"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: problem of  information">edit</a>]</span> <span class="mw-headline">problem of information</span></h2>
<p>I just added a section about (well-known) problem of conveying information with probabilities; I think it should be better integrated with the rest, but I am not an expert in the rest.&#160;:) <a href="../../../s/a/m/User%7ESamohyl_Jan_6337.html" title="User:Samohyl Jan">Samohyl Jan</a> 19:59, 11 Feb 2005 (UTC)</p>
<dl>
<dd>Looks interesting, but can it be revised to either use "information" in the technical sense, or some other word if not? It seems like we should omit the disclaimer <i>note that the use of "information" here is not a strict mathematical one</i> if possible. Regards &amp; happy editing, <a href="../../../w/i/l/User%7EWile_E._Heresiarch_9879.html" title="User:Wile E. Heresiarch">Wile E. Heresiarch</a> 00:37, 12 Feb 2005 (UTC)</dd>
</dl>
<dl>
<dd>
<dl>
<dd>You're right. I have changed "information" to "evidence", and it looks much better now. <a href="../../../s/a/m/User%7ESamohyl_Jan_6337.html" title="User:Samohyl Jan">Samohyl Jan</a> 10:16, 12 Feb 2005 (UTC)</dd>
</dl>
</dd>
</dl>
<p>Oh, man. People really shouldn't write sections when they have no clue what they are talking about. A bayesian would assign the same probability of picking a head, however the prior probability over hypotheses (what is the probability of the coin landing heads), would most likely be represented as a Beta distribution whose distribution is</p>
<p>P(theta;a,b) = Gamma(a+b)/Gamma(a)Gamma(b) * theta^a-1 (1-theta)^b-1</p>
<p>I'm going to place a NPOV message on this article until these errors are sorted out.</p>
<dl>
<dd>why don't you just fix it? Why the fuss?<a href="../../../b/a/n/User%7EBanno_733d.html" title="User:Banno">Banno</a> 06:06, August 27, 2005 (UTC)</dd>
</dl>
<dl>
<dd>I think the section is about that there is other uncertainty - ignorance - than probability, and as such it cannot be represented by the probability distribution itself. You could of course represent both as a probability over the space of hypotheses, but, imho, this is exactly the core of the frequentist/bayesian debate about what can/should probabilities represent. You chose <a href="../../../b/e/t/Beta_distribution.html" title="Beta distribution">beta distribution</a> for technical reasons (<a href="../../../c/o/n/Conjugate_prior.html" title="Conjugate prior">conjugate prior</a>), but in fact, you're in this case only interested in the mean and variance of the probability distribution, so many other distributions would do. This notion is exactly what <a href="../../../d/e/m/Dempster-Shafer_theory_4f8f.html" title="Dempster-Shafer theory">Dempster-Shafer theory</a> or upper probability tries to model. On the other hand, you're right, from a practical POV, bayesians commonly do it this way, and it (probably) yields the same results as using Dempster-Shafer theory (except I am not sure if you won't run into computational difficulties in more complex cases than coin flipping). But I am not expert, and will happily read opinion of the others. <a href="../../../s/a/m/User%7ESamohyl_Jan_6337.html" title="User:Samohyl Jan">Samohyl Jan</a> 13:21, 27 August 2005 (UTC)</dd>
</dl>
<dl>
<dd>
<dl>
<dd>I'm not solely interested in the mean and variance. In fact, if I were going to determine P(something), where something depended on theta then I would treat theta as a nuisance parameter and integrate it out, taking every piece of information conveyed in the distribution into account. The inappropriate thing about the section (and the reason I fixed it) is that this page is about Bayesian probability, and in Bayesian probability, you can express uncertainty in hypothesis using probability theory. I think that, eventually, these pages on Bayesian probability and inference should be rearranged and that a new page on the war between frequentists and bayesians be separated out. It's important to know that there is controversy, but the back-and-forth "this is what bayesians think but..." makes an already difficult subject matter even more difficult.</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>Now, Samohyl. You say that "this is exactly the core of the debate." I agree. However, we cannot have the articles flopping back and forth. Bayesians choose to interpret probabilities as degrees of beliefs. Frequentists don't. So your section on how frequentists point out some shortfall of Bayesian analysis, when in fact, that's NOT how bayesians DO IT, is absurd. Frequentists think its a shortfall because they don't allow the solution Bayesians use.</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>I think we should hear some suggestions about what might go on a "controversy page"... My suggestion would be a page on "Frequentists paradoxes", "Bayesian paradoxes", "Bayesian vs Frequentist", and then change the Bayesian pages to cite that there is controversy but then explain clearly the point of view of Bayesians (and similarly on the frequentist pages). roydanroy 13:48, 27 August 2005 (EST)</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Fine. I just found <a href="../../../p/r/o/Probability_interpretations.html" title="Probability interpretations">probability interpretations</a>, which seems to be the right page. <a href="../../../s/a/m/User%7ESamohyl_Jan_6337.html" title="User:Samohyl Jan">Samohyl Jan</a> 20:36, 27 August 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p><a name="Disputed:_Paradoxes_in_Bayesian_inference" id="Disputed:_Paradoxes_in_Bayesian_inference"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Disputed: Paradoxes in Bayesian inference">edit</a>]</span> <span class="mw-headline">Disputed: Paradoxes in Bayesian inference</span></h2>
<p>I would dispute the following:</p>
<p><i>This can lead to paradox situations such as false positives in bayesian inference, where your prior probability can be based on more evidence (contains more information) than your posterior probability, if the conditional probability is not based on enough evidence.</i></p>
<p>What evidence is there that paradoxes occur? User:Blaise 21:11, 2 Mar 2005 (UTC)</p>
<dl>
<dd>You are right, I thought there was a connection, but it isn't, so I will remove the sentence. Thanks for correction. But some example where this is a problem would be fine, if there is any. <a href="../../../s/a/m/User%7ESamohyl_Jan_6337.html" title="User:Samohyl Jan">Samohyl Jan</a> 19:54, 3 Mar 2005 (UTC)</dd>
</dl>
<p><a name="Another_dispute:_What_Laplace_meant" id="Another_dispute:_What_Laplace_meant"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Another dispute: What Laplace meant">edit</a>]</span> <span class="mw-headline">Another dispute: What Laplace meant</span></h2>
<p>In the history section, this apparent paraphrasing of Laplace bugs me:</p>
<dl>
<dd><i>'It is a bet of 11000 to 1 that the error in this result is not within 1/100th of its value'</i></dd>
</dl>
<p>Am I reading this wrong, or is that saying exactly the opposite of what is mean? Shouldn't "not within" be replaced by either "within" or "not more than" in this context? Or am I reading the stating of the odds backwards? I traced it all the way back to <a href="http://en.wikipedia.org/w/index.php?title=Bayesian_probability&amp;diff=52675&amp;oldid=52285" class="external text" title="http://en.wikipedia.org/w/index.php?title=Bayesian_probability&amp;diff=52675&amp;oldid=52285" rel="nofollow">the edit</a> which added that whole section, so it's not just some random minor vandal sticking a "not" in there, at least. --<a href="../../../j/o/h/User%7EJohnOwens_79a2.html" title="User:JohnOwens">John Owens</a> <a href="../../../j/o/h/User_talk%7EJohnOwens_c414.html" title="User talk:JohnOwens">(talk)</a> 23:28, 2005 Mar 14 (UTC)</p>
<dl>
<dd>I am morally certain that you are correct. The sentence doesn't make sense as written, and doesn't comport with my memory of what Laplace wrote. However, I cannot find the original quote in my library. Stigler doesn't have it, nor does J. O. Berger (at least, not so I can find it from the index). I had thought that Jaynes might have it, but the index in his finally printed version is not very good. I would be comfortable in making the change, but think that if someone can find the quote we would be on more solid ground. Perhaps the original contributor of this quotation can verify it? <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 20:34, 29 August 2005 (UTC)</dd>
</dl>
<p>I believe I have found where the original contributor got this. The quotation from the Wiki article is</p>
<dl>
<dd>For instance, Laplace estimated the mass of Saturn, given orbital data that were available to him from various astronomical observations. He presented the result together with an indication of its uncertainty, stating it like this: 'It is a bet of 11000 to 1 that the error in this result is not within 1/100th of its value'. Based on current estimates, he would have won the bet, as another 150 years' accumulation of data has changed the estimate by only 0.63%.</dd>
</dl>
<p>D. S. Sivia, in <i>Data Analysis: A Bayesian Tutorial</i> (Oxford: Clarendon Press 1996, pp. 7-8) writes</p>
<dl>
<dd>Laplace stated that '...it is a bet of 11,000 to 1 that the error of this result is not 1/100 of its value'. He would have won the bet, as another 150 years' accumulation of data has changed the estimate by only 0.63%!</dd>
</dl>
<p>The only significant difference is the inclusion of the word 'within' in the Wiki article, which appears to be a mistake. Accordingly, I will remove that word from the article.</p>
<p>However, the second sentence now becomes problematic, as it appears to have been lifted from Sivia's book without attribution and only minor change. I do not know what the Wiki convention for this would be. It seems to me that a paraphrase is in order, or else an attribution to Sivia. Can someone with more experience with Wikipedia comment? <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 23:05, 29 August 2005 (UTC)</p>
<dl>
<dd>Hello Bill. I'm glad to see you're here. I am familiar with your posts to sci.stat.math (mine appear under the name "Robert Dodier"). Anyway, about the unattributed quotation, we do try to keep our collective noses clean wrt copyrights. I see two solutions: (1) "put it in quote marks" and attribute the remark to Sivia. (2) replace it with a paraphrase. Maybe someone has already done one or the other. Thanks for your contributions and keep up the good work! <a href="../../../w/i/l/User%7EWile_E._Heresiarch_9879.html" title="User:Wile E. Heresiarch">Wile E. Heresiarch</a> 01:12, 30 August 2005 (UTC)</dd>
</dl>
<p>Hello Robert, I remember you well (but have stopped reading UseNet as it was taking too much of my time). Thanks for your remarks. I have modified the article to explicitly credit Sivia with the comment, and have added his book in the references section.</p>
<p>Meantime, if you (or anyone else) wishes to look at the article I wrote on <a href="../../../a/d/m/Admissible_decision_rules.html" title="Admissible decision rules">admissible decision rules</a> or the additions I made to the section on <a href="../../../p/r/i/Prior_probability.html#Uninformative_priors" title="Prior probability">uninformative priors</a>, I would be grateful for any additions or corrections. Also, articles are needed on reference priors, complete class theorems and Stein effect. Is anyone willing to take a crack at some of these? <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 20:43, 2 September 2005 (UTC)</p>
<p><a name="Need_a_better_example_of_frequentist_probability" id="Need_a_better_example_of_frequentist_probability"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Need a better example of frequentist probability">edit</a>]</span> <span class="mw-headline">Need a better example of frequentist probability</span></h2>
<p>In the section titled Bayesian and Frequentist probability, the statement is:</p>
<dl>
<dd><i>'For example, Laplace estimated the mass of Saturn (described above) in this way. According to the frequency probability definition, however, the laws of probability are not applicable to this problem. This is because the mass of Saturn is a constant and not a random variable, therefore, it has no frequency distribution and so the laws of probability cannot be used.'</i></dd>
</dl>
<p>My reading of this is that the statement refers to the value for the mass arrived at experimentally, not the absolute mass of Saturn. The experimental results will have a frequency distribution, depending on the degree of error associated with the method and apparatus used. I don't have an alternative example, but I find this one less than ideal.</p>
<dl>
<dd>
<dl>
<dd><b>WRONG WRONG WRONG WRONG!!!!!!</b> This is a good example precisely because it's about the absolute mass and that is NOT a random variable. The whole point of Bayesianism is to assign probability distributions <b>NOT</b> to things that <b>vary randomly</b>, but to things that are <b>uncertain</b>. <a href="../../../m/i/c/User%7EMichael_Hardy_e932.html" title="User:Michael Hardy">Michael Hardy</a> 23:17, 28 August 2005 (UTC)</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Note that some authorities say that the essence of Bayesianism is that quantities such as the mass of Saturn <i>are</i> considered to be random variables, with the data fixed and not themselves random variables (although a particular data set does arise from a random process) whereas under frequentism it is the other way around. Much of the Bayesian literature talks in this way. But I am in agreement with Michael Hardy that the Saturn mass example is an excellent one (besides being a classical example that is commonly given as such). <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 20:16, 29 August 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Indeed, James Berger is one of the Bayesian authorities from whom you may hear that, and he will tell you explicitly that he really doesn't care about this particular issue, so at least on that point he is not an authority. Other "Bayesian authorities" use the language of "random variables" merely because it's conventional in probability theory, and it takes considerable effort to extensively revise conventional nomeclature to be consistent with one's philosophical position. Bottom line: those "Bayesian authorities" who say things like that are not necessarily taking a well-thought-out philosophical position; they're just being conventional. But see <a href="../../../e/d/w/Edwin_Jaynes_b9e7.html" title="Edwin Jaynes">Edwin Jaynes</a>'s much-discussed book <i>Probability Theory: the Logic of Science</i>. <a href="../../../m/i/c/User%7EMichael_Hardy_e932.html" title="User:Michael Hardy">Michael Hardy</a> 01:17, 30 August 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>I asked Jim Berger about exactly this point not too long ago. He agreed that from the Bayesian POV, parameters are properly considered random variables. He didn't volunteer that he doesn't particularly care about the issue, and I didn't ask. I'm puzzled why you would think that his not caring (if this is indeed the case) would make him not an authority! But in any case, from an axiomatic point of view, parameters in Bayesian statistics possess all of the characteristics of <a href="../../../r/a/n/Random_variables.html" title="Random variables">random variables</a>, so it seems hard to avoid concluding that they are. Of course, if you are using the phrase 'random variable' in a vernacular sense ("things that vary randomly") then they aren't, but I don't see a good reason to abandon a precise mathematical definition just because the general public uses 'random' in an imprecise way.</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>I have the published version of Jaynes' book (Cambridge Press, 2003)...can you cite a page in particular? I don't want to have to search the entire book, and the index isn't very good. <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 19:38, 2 September 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>From a <i>mathematical</i> point of view, they are "random variables". I'll try to find an appropriate page in the Jaynes book, but until then, look in the index under "mind projection fallacy". <a href="../../../m/i/c/User%7EMichael_Hardy_e932.html" title="User:Michael Hardy">Michael Hardy</a> 00:00, 3 September 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>The word random is avoided by Bayesians because most things are not random (and some think nothing is). Jaynes goes through a lot of trouble explaining that a coin flip's result is <i>uncertain</i> not because coin flips are random and unpredictable, but instead, that the <i>initial conditions</i> of the coin flip (which 100% characterize its final head/tail state) are uncertain and that this uncertainty propagates and results in uncertainty in its final resting state. People who do bayesian analysis but still use the frequentist language (random variables, IID, etc) sometimes are still confused about this issue, falling prey to Jaynes' <i>mind projection fallacy</i> as Mr. Hardy cited. Roydanroy</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p><a name="Beta_Prior_Distribution_at_Bayesian_data_analysis_example" id="Beta_Prior_Distribution_at_Bayesian_data_analysis_example"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Beta Prior Distribution at Bayesian data analysis example">edit</a>]</span> <span class="mw-headline">Beta Prior Distribution at Bayesian data analysis example</span></h2>
<p>Hi,</p>
<p>When "probability that the next ball is black is p" is calculated under situation 1 (no knowledge as to the quantities) a Beta prior distribution is calculated "B(alfa_B=1,alfa_W=1)". Nevertheless when I checked Beta distribution definition at "<a href="http://en.wikipedia.org/wiki/Beta_distribution" class="external free" title="http://en.wikipedia.org/wiki/Beta_distribution" rel="nofollow">http://en.wikipedia.org/wiki/Beta_distribution</a>" it doesn't correspond. Fita should be powered to "alfa_B-1" and "1-fita" should be powered to "alfa_W-1" which shoud be in our case "0" for both cases. This is not what is shown at the example where both values are powered to "1".</p>
<p>I also think there is a type mistake at same point. Later it refers to the case where m black balls and n white balls are drawed and it is defined "alfa_B=1+m" and "alfa_B=1+n". I assume that second case should be "alfa_W=1+n".</p>
<p>Besides that it would help if same notation is used everywhere ("alfa_B" should be "alfa" and "alfa_W" should be "beta").</p>
<p>I hope it is clear enough. Sorry for my English.</p>
<p>Regards</p>
<p>Alex</p>
<dl>
<dd>You're right. Although some authorities define the beta distribution as in this page, the most standard definition incorporates a '-1' in the exponents of both <span class="texhtml">α</span> and <span class="texhtml">β</span>. Also, the Wiki article on the <a href="../../../b/e/t/Beta_distribution.html" title="Beta distribution">beta distribution</a> defines it in this way. So for uniformity, the article on <a href="../../../b/a/y/Bayesian_probability.html" title="Bayesian probability">Bayesian probability</a> should be revised to conform. Note that the normalizing constant also needs correction as per the <a href="../../../b/e/t/Beta_distribution.html" title="Beta distribution">beta distribution</a> article. Are there any disagreements? <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 17:08, 4 September 2005 (UTC)</dd>
</dl>
<dl>
<dd>
<dl>
<dd>Thanks (good eye). I've fixed it.</dd>
</dl>
</dd>
</dl>
<p>Hi again,</p>
<p>I'm not so sure that this new equation "P(fita)=1" regardless fita value is correct. It implies that statement that the probability that the next ball is black is ex. 30% is 100%, also that statement that the probability that the next ball is black is 60% is also 100%, ... it would make more sense to me if only P(0.5)=1 and all other values 0, what do you think?</p>
<p>Alex</p>
<dl>
<dd>I assume by 'fita' you mean 'theta' or in symbols either <span class="texhtml">θ</span> or <span class="texhtml">Θ</span>. But nowhere in the article can I find that anyone is asserting that <span class="texhtml">θ = 1</span>. In example three it is suggested that when you know how many balls are in the box (and that the number of black balls equals the number of white balls), that an appropriate prior is <img class='tex' src="../../../math/f/9/8/f9889222484e61f7ed6126a4b8b9db9b.png" alt="P(\Theta)=\delta(\frac{1}{2})" />, though I think the writer meant <img class='tex' src="../../../math/e/c/5/ec542f4f86059defa9d7c514e267f46f.png" alt="P(\theta)=\delta(\frac{1}{2})" /> since <span class="texhtml">Θ</span> isn't defined anywhere (someone check me on this). I think this isn't quite right...rather, it should be written <img class='tex' src="../../../math/e/5/a/e5aba607f0732aa018f330ab25ae3e9b.png" alt="p(\theta)=\delta(\theta-\frac{1}{2})" />, which puts a unit mass on <span class="texhtml">θ = 1 / 2</span> and no mass anywhere else. The lower case <span class="texhtml"><i>p</i></span> is appropriate since the <a href="../../../d/i/r/Dirac_delta_function.html" title="Dirac delta function">Dirac delta function</a> is a (limiting) <a href="../../../p/r/o/Probability_density.html" title="Probability density">probability density</a> rather than a distribution on a finite space. <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 16:54, 9 September 2005 (UTC)</dd>
</dl>
<dl>
<dd>
<dl>
<dd>Hi Bill, You are right, I meant theta "<span class="texhtml">θ</span>" Sorry&#160;:). My question was on example 1 "You have a box with white and black balls, but no knowledge as to the quantities". After last editorial modification it states that <span class="texhtml"><i>P</i>(θ) = 1</span> letting <span class="texhtml">θ = <i>p</i></span> represent the statement "the probability that the next ball is black is p". From that, I conclude that independently to <span class="texhtml">θ</span> value (from 0 to 1) probability of <span class="texhtml">θ</span> is always 1. So, regardless the probability value (ex. p=0.3, p=0.9, ...) a Bayesian will assign a 100% probability to the sentence. That doesn't make sense to me ... It makes sense to me that all sentences (with different "p" values") may have the same probability value since all can be equally right, except for values p=0 (since we know there are black balls in the box) and p=1 (since we know there are white balls in the box), but, to me that probability value cannot be 1. Which are your thoughts? Besides that I agree with you, <img class='tex' src="../../../math/e/5/a/e5aba607f0732aa018f330ab25ae3e9b.png" alt="p(\theta)=\delta(\theta-\frac{1}{2})" /> is more correct. Bet regards, Alex</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>I am missing something here. I do not see where it states, after example 1, after the last editorial modification, where it says that <span class="texhtml"><i>P</i>(θ) = 1</span>. If it were to say this, it would certainly be wrong in the context of the example. But I cannot find this statement. Even when I search the source code for the article for 'P(\theta)=1', I cannot find it.</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Please show me specifically where the statement is. <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 01:52, 10 September 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Example 1 states <img class='tex' src="../../../math/b/e/3/be3bed123e9613e714883496d520e278.png" alt="P(\theta) = \Beta(\alpha_B=1,\alpha_W=1) = \frac{\Gamma(\alpha_B + \alpha_W)}{\Gamma(\alpha_B)\Gamma(\alpha_W)}\theta^{\alpha_B-1}(1-\theta)^{\alpha_W-1} = \frac{\Gamma(2)}{\Gamma(1)\Gamma(1)}\theta^0(1-\theta)^0=1" />, so, at the end it is <span class="texhtml"><i>P</i>(θ) = 1</span></dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>OK, now I see the confusion. The statement indeed says that <span class="texhtml"><i>P</i>(θ) = 1</span>, but that is not a statement that <span class="texhtml">θ = 1</span> dogmatically. Rather, it is a statement that <span class="texhtml">θ</span> can take on <i>any</i> value in its range, which for the beta distribution is the interval [0,1]. Equal probability is assigned to each value in the range. Recall that <span class="texhtml"><i>P</i>(θ)</span> is a <a href="../../../p/r/o/Probability_density_function.html" title="Probability density function">probability density function</a>. The value 1 is just the normalization (since the integral over all of the range of <span class="texhtml">θ</span> has to equal 1). In another case, say where <span class="texhtml">θ</span> ranges from -1 to 1 (for example, a correlation coefficient), one would have range [-1,1] and <span class="texhtml"><i>P</i>(θ) = 1 / 2</span> so that the integral would be 1.</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>So, there is nothing wrong with this equation. It is exactly correct.</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Does this clarify things for you? <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 13:54, 10 September 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Hi, I've got to the same conclusion this afternoon while I was driving. <span class="texhtml"><i>P</i>(θ) = 1</span> means that all statements (all <span class="texhtml">θ</span> values) have the same probability. But it doesn't mean that they have 100% probability. I only would like to emphasize that only <span class="texhtml">θ = 0</span> and <span class="texhtml">θ = 1</span> values should have 0% probability since we know for sure that in the box there are black and white balls. Regards. Alex</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Technically, if you know that there are both black balls and white balls in the box, the probability is zero for <img class='tex' src="../../../math/d/f/d/dfdf93e843d47e095fde120e8c7c0976.png" alt="\theta\in\{0,1\}" />. This would mean that <span class="texhtml"><i>P</i>(θ) = 1</span> only on the open interval (0,1) but not on the closed interval [0,1]. However, this is only a technicality, since probabilities are derived by integration over intervals and generally speaking (for well-behaved functions) the integrals are the same regardless of whether one includes or excludes the endpoints.</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>To make things even more complicated, probabilities are defined on a <a href="../../../s/i/g/Sigma-algebra.html" title="Sigma-algebra">sigma-algebra</a> of sets, and for the real line these are usually defined as sets that are half-open and half-closed, e.g., [a,b). The technicalities of constructing such a sigma-algebra may lead to odd looking things happening at the endpoints of intervals. But, as I say, these technicalities need not concern anyone but the most fastidious, since they don't arise under most well-behaved circumstances. <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 22:31, 10 September 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Do last editing modifications on example 1 affect earlier sentence "...they would choose probability 1/2 in all three cases..." refering to Bayesian? Alex</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>No <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 02:54, 12 September 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p><br /></p>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>Hi, at example 1, "...<span class="texhtml">α<sub><i>B</i></sub> = 1 + <i>m</i></span>, <span class="texhtml">α<sub><i>B</i></sub> = 1 + <i>n</i></span>...." should read "...<span class="texhtml">α<sub><i>B</i></sub> = 1 + <i>m</i></span>, <span class="texhtml">α<sub><i>w</i></sub> = 1 + <i>n</i></span>", shoudn't it? Alex</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p><br /></p>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd>You're right. I've fixed this. <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 15:14, 19 September 2005 (UTC)</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p><a name="A_good_overview_for_.22dummies.22_would_be_nice." id="A_good_overview_for_.22dummies.22_would_be_nice."></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: A good overview for &quot;dummies&quot; would be nice.">edit</a>]</span> <span class="mw-headline">A good overview for "dummies" would be nice.</span></h2>
<p>I read this article and had a little trouble deciphering exactly what Bayesian probability is. This article seems like a good body of knowledge for people who already know about this topic. I found a good overview on the web at <a href="http://www.bayesian.org/openpage.html" class="external free" title="http://www.bayesian.org/openpage.html" rel="nofollow">http://www.bayesian.org/openpage.html</a> (Overview Section) and then it follows up with <a href="http://www.bayesian.org/bayesexp/bayesexp.htm" class="external free" title="http://www.bayesian.org/bayesexp/bayesexp.htm" rel="nofollow">http://www.bayesian.org/bayesexp/bayesexp.htm</a> . This type of simplified explanation is exactly what I think is missing from this article. <small>—The preceding <a href="../../../s/i/g/Wikipedia%7ESign_your_posts_on_talk_pages_ee53.html" title="Wikipedia:Sign your posts on talk pages">unsigned</a> comment was added by A67676767 (talk • <a href="../../../c/o/n/Special%7EContributions_A67676767_8ffe.html" title="Special:Contributions/A67676767">contribs</a>) 18 December 2005.</small></p>
<dl>
<dd>I agree. I'm doing an assignment about errors in information systems and I need a paragraph summing up Bayesian probability. This article was useless to me.--195.194.178.251 16:07, 6 March 2007 (UTC)</dd>
</dl>
<p>Technical tag added, this is something only an expert can understand SeanWDP 20:27, 24 March 2007 (UTC)</p>
<p><a name="Bogus_Portrait.3F" id="Bogus_Portrait.3F"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Bogus Portrait?">edit</a>]</span> <span class="mw-headline">Bogus Portrait?</span></h2>
<p>Note that the portrait of Thomas Bayes in the article is believed by most authorities not to be of Thomas Bayes, although the portrait is widely used. Don't know what to do about this. At the very least, the fact that the authenticity of the portrait is disputed should be noted. <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Bill Jefferys</a> 23:28, 30 January 2006 (UTC)</p>
<p><a name="Bayesian_and_frequentist_probability" id="Bayesian_and_frequentist_probability"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Bayesian and frequentist probability">edit</a>]</span> <span class="mw-headline">Bayesian and frequentist probability</span></h2>
<p>I find this comment somewhat unclear: <i>The Bayesian approach is in contrast to the concept of frequency probability where probability is held to be derived from observed or imagined frequency distributions or proportions of populations.</i> If you allow someone to invent some imagined frequency distribution, then how can you rule out them imagining a multiverse, each member of which has a Saturn of different mass (distributed according to some prior)? <a href="../../../j/d/a/User%7EJdannan_1714.html" title="User:Jdannan">Jdannan</a> 13:19, 2 February 2006 (UTC)</p>
<dl>
<dd>I have changed "imagined" distributions to "predicted" frequency distributions in the article; but I don't know whether that helps.</dd>
</dl>
<dl>
<dd>
<dl>
<dd>I've changed it to "defined". It seems to me that a very clear division can be made over whether the prior is explicitly (implicitly) provided as part of the problem, or must be chosen by the analyst. If someone asks me for the probability of rainfall on a random February day, it would be natural to use an (essentially frequentist) analysis of historical data. If someone asks me for the probability of rain <b>tomorrow</b>, there is no obvious prior and I have to make some judgements.<a href="../../../j/d/a/User%7EJdannan_1714.html" title="User:Jdannan">Jdannan</a> 00:16, 4 February 2006 (UTC)</dd>
</dl>
</dd>
</dl>
<dl>
<dd>Such imagined distributions, or <a href="../../../s/t/a/Statistical_ensemble.html" title="Statistical ensemble">statistical ensembles</a> as they are called, are widely used by both Bayesians and frequentists to mentally visualise probabilities. The basic issue is that frequentists deny that you can meaningfully form a prior for the mass of Saturn in any objective way; on the other hand, if you know the (observable) pattern of random errors introduced by your instruments, you can form an objective law of probability for the results of your observations for a given fixed unknown parameters. And this set of probabilities would be falsifiable: it would reflect the predicted spread of repeated observations, which you could check by performing the experiment again and again. On the other hand there is only one Saturn, so (from a frequentist's point of view), any <i>probability distribution</i> for it is unobjective, unfalsifiable, and therefore meaningless. For the frequentist it is only probabilities which correspond to predicted frequencies of observations that you could in principle perform that can have any meaning. -- <a href="../../../j/h/e/User%7EJheald_bb56.html" title="User:Jheald">Jheald</a> 14:32, 2 February 2006 (UTC).</dd>
</dl>
<p><b>FOUNDATIONS</b> As is known, the idea of a probabilistic logic of inductive inference based on some form of the Principle of Indifference always retained a powerful appeal. Keynes recommended a modified version of the principle in order to achieve this aim. Carnap followed Keynes in this attempt of creating a purely logical theory of induction. However, up to now all modifications of the principle failed. A modified version of the Principle of Indifference may be provided without generating paradoxes and inconsistencies. Besides, a general criterion of assignment of prior probabilities in case of initial ignorance may be suggested, thus providing a reply to the objections to the so-called objective Bayesianism. The new (or modified) principle of indifference prescribes a uniform distribution over a partition of hypotheses where there is no reason to believe one more likely to be true than any other, in the sense of both irrelevance of prior information and impartiality of design d (or method of inquiry). In order to ensure the impartiality of d with respect to the parameter theta, it is sufficient that, for all possible likelihood functions that can be obtained from d, the respective maxima of likelihood remain constant with respect to theta itself. In other words, the prior is uniform if the maximum of each curve is constant (or it is situated on the same level of any other). Besides, we can assume the prior for a parameter proportional to the corresponding maximum value of likelihood for all possible likelihood functions obtainable from the projected design. cf. de Cristofaro, <i>Foundations of the Objective Bayesian Inference</i> This paper was presented to the First Symposium on Philosophy, History and Methodology of ERROR held to Virginia Tech in Blacksburg (Virginia) on 1-5 June 2006. Available on INTERNET. <small>—The preceding <a href="../../../s/i/g/Wikipedia%7ESignatures_bcf8.html" title="Wikipedia:Signatures">unsigned</a> comment was added by 150.217.32.23 (<a href="../../../1/5/0/User_talk%7E150.217.32.23.html" title="User talk:150.217.32.23">talk</a> • <a href="../../../c/o/n/Special%7EContributions_150.217.32.23_d9b6.html" title="Special:Contributions/150.217.32.23">contribs</a>) 20 June 2006.</small>, <small>possibly Rodolfo de Cristofaro (signed that way elsewhere)</small>.</p>
<p><a name="Credence:_is_it_a_synonym_for_subjective_probability" id="Credence:_is_it_a_synonym_for_subjective_probability"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: Credence: is it a synonym for subjective probability">edit</a>]</span> <span class="mw-headline">Credence: is it a synonym for subjective probability</span></h2>
<p>The classic statement of the <a href="../../../s/l/e/Sleeping_Beauty_problem_f0fa.html" title="Sleeping Beauty problem">Sleeping Beauty problem</a> employs the term <i><a href="../../../c/r/e/Credence.html" title="Credence">credence</a></i>, apparently as a synonym for <i>subjective probability</i>. I have been unable to find convincing examples of the word in this sense on the internet. The <a href="../../../o/x/f/Oxford_English_Dictionary_872c.html" title="Oxford English Dictionary">Oxford English Dictionary</a> of 1933 offers 8 senses, of which only the first means belief, and does not havve a shade of meaning of <i>degree of belief</i>. Do other contributors regard <i>credence</i> as a technical term, or was the formulation of the problem unfortunate? <a href="../../../p/j/t/User%7EPJTraill_4845.html" title="User:PJTraill">PJTraill</a> 00:50, 6 November 2006 (UTC)</p>
<dl>
<dd>For some reason this problem (Sleeping Beauty) is usually stated using the word "credence." Maybe Adam Elga (the inventor) wanted to emphasize the subjective nature of his question in a theory-neutral manner—thus avoiding theory-loaded expressions like "subjective probability" and "Bayesianism"? <a href="../../../i/n/i/User%7EINic_5861.html" title="User:INic">INic</a> 04:03, 6 November 2006 (UTC)</dd>
</dl>
<p><a name="subject_matter" id="subject_matter"></a></p>
<h2><span class="editsection">[<a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html" title="Edit section: subject matter">edit</a>]</span> <span class="mw-headline">subject matter</span></h2>
<p>The article doesn't clearly state what "Bayesian probability" is. It should probably be contrasted to frequency-based probability. This is not done anywhere, not even in the section claiming to do it in its name. --<a href="../../../m/a/r/User%7EMarSch_3340.html" title="User:MarSch">MarSch</a> 12:49, 18 February 2007 (UTC)</p>

<!-- 
Pre-expand include size: 2893 bytes
Post-expand include size: 879 bytes
Template argument size: 151 bytes
Maximum: 2048000 bytes
-->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org../../../b/a/y/Talk%7EBayesian_probability_b4f7.html">http://en.wikipedia.org../../../b/a/y/Talk%7EBayesian_probability_b4f7.html</a>"</div>
	    <div id="catlinks"><p class='catlinks'><a href="../../../c/a/t/Special%7ECategories_101d.html" title="Special:Categories">Category</a>: <span dir='ltr'><a href="../../../w/i/k/Category%7EWikipedia_articles_that_are_too_technical_8d88.html" title="Category:Wikipedia articles that are too technical">Wikipedia articles that are too technical</a></span></p></div>	    <!-- end content -->
	    <div class="visualClear"></div>
	  </div>
	</div>
      </div>
      <div id="column-one">
	<div id="p-cactions" class="portlet">
	  <h5>Views</h5>
	  <ul>
	    <li id="ca-nstab-main"
	       	       ><a href="../../../b/a/y/Bayesian_probability.html">Article</a></li><li id="ca-talk"
	       class="selected"	       ><a href="../../../b/a/y/Talk%7EBayesian_probability_b4f7.html">Discussion</a></li><li id="ca-current"
	       	       ><a href="http://en.wikipedia.org/wiki/Talk:Bayesian_probability">Current revision</a></li>	  </ul>
	</div>
	<div class="portlet" id="p-logo">
	  <a style="background-image: url(../../../images/wiki-en.png);"
	    href="../../../index.html"
	    title="Main Page"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
		<div class='portlet' id='p-navigation'>
	  <h5>Navigation</h5>
	  <div class='pBody'>
	    <ul>
	    	      <li id="n-Main-page"><a href="../../../index.html">Main page</a></li>
	     	      <li id="n-Contents"><a href="../../../c/o/n/Wikipedia%7EContents_3181.html">Contents</a></li>
	     	      <li id="n-Featured-content"><a href="../../../f/e/a/Wikipedia%7EFeatured_content_24ba.html">Featured content</a></li>
	     	      <li id="n-currentevents"><a href="../../../c/u/r/Portal%7ECurrent_events_bb60.html">Current events</a></li>
	     	    </ul>
	  </div>
	</div>
		<div class='portlet' id='p-interaction'>
	  <h5>interaction</h5>
	  <div class='pBody'>
	    <ul>
	    	      <li id="n-About-Wikipedia"><a href="../../../a/b/o/Wikipedia%7EAbout_8d82.html">About Wikipedia</a></li>
	     	      <li id="n-portal"><a href="../../../c/o/m/Wikipedia%7ECommunity_Portal_6a3c.html">Community portal</a></li>
	     	      <li id="n-contact"><a href="../../../c/o/n/Wikipedia%7EContact_us_afd6.html">Contact us</a></li>
	     	      <li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Fundraising">Make a donation</a></li>
	     	      <li id="n-help"><a href="../../../c/o/n/Help%7EContents_22de.html">Help</a></li>
	     	    </ul>
	  </div>
	</div>
		<div id="p-search" class="portlet">
	  <h5><label for="searchInput">Search</label></h5>
	  <div id="searchBody" class="pBody">
	    <form action="javascript:goToStatic(3)" id="searchform"><div>
	      <input id="searchInput" name="search" type="text"
	        accesskey="f" value="" />
	      <input type='submit' name="go" class="searchButton" id="searchGoButton"
	        value="Go" />
	    </div></form>
	  </div>
	</div>
	      </div><!-- end of the left (by default at least) column -->
      <div class="visualClear"></div>
      <div id="footer">
    <div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="../../../skins/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>	<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="../../../images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>	<ul id="f-list">
	  	  	  <li id="f-credits">This page was last modified 20:27, 24 March 2007 by Wikipedia user SeanWDP. Based on work by Wikipedia user(s) <a href="../../../m/a/r/User%7EMarSch_3340.html" title="User:MarSch">MarSch</a>, <a href="../../../e/d/j/User%7EEdJohnston_759e.html" title="User:EdJohnston">EdJohnston</a>, <a href="../../../i/n/i/User%7EINic_5861.html" title="User:INic">INic</a>, <a href="../../../p/j/t/User%7EPJTraill_4845.html" title="User:PJTraill">PJTraill</a>, Vizier, <a href="../../../b/i/l/User%7EBilz0r_6726.html" title="User:Bilz0r">Bilz0r</a>, <a href="../../../j/d/a/User%7EJdannan_1714.html" title="User:Jdannan">Jdannan</a>, <a href="../../../j/h/e/User%7EJheald_bb56.html" title="User:Jheald">Jheald</a>, <a href="../../../b/i/l/User%7EBilljefferys_1728.html" title="User:Billjefferys">Billjefferys</a>, A67676767, Roydanroy, <a href="../../../m/i/c/User%7EMichael_Hardy_e932.html" title="User:Michael Hardy">Michael Hardy</a>, <a href="../../../w/i/l/User%7EWile_E._Heresiarch_9879.html" title="User:Wile E. Heresiarch">Wile E. Heresiarch</a>, <a href="../../../s/a/m/User%7ESamohyl_Jan_6337.html" title="User:Samohyl Jan">Samohyl Jan</a>, <a href="../../../b/a/n/User%7EBanno_733d.html" title="User:Banno">Banno</a>, Gavin Crooks, <a href="../../../j/o/h/User%7EJohnOwens_79a2.html" title="User:JohnOwens">JohnOwens</a>, <a href="../../../d/o/g/User%7EDogface_68d5.html" title="User:Dogface">Dogface</a>, <a href="../../../g/r/i/User%7EGrick_595e.html" title="User:Grick">Grick</a>, <a href="../../../c/y/a/User%7ECyan_b70d.html" title="User:Cyan">Cyan</a>, ( and <a href="../../../l/a/r/User%7ELarry_Sanger_6200.html" title="User:Larry Sanger">Larry Sanger</a> and Anonymous user(s) of Wikipedia.</li>	  <li id="f-copyright">All text is available under the terms of the <a class='internal' href="../../../t/e/x/Wikipedia%7EText_of_the_GNU_Free_Documentation_License_702a.html" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="../../../c/o/p/Wikipedia%7ECopyrights_92c4.html" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc</a>., a US-registered <a class='internal' href="../../../5/0/1/501%28c%29.html#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="../../../n/o/n/Non-profit_organization.html" title="Non-profit organization">nonprofit</a> <a href="../../../c/h/a/Charitable_organization.html" title="Charitable organization">charity</a>.<br /></li>	  <li id="f-about"><a href="../../../a/b/o/Wikipedia%7EAbout_8d82.html" title="Wikipedia:About">About Wikipedia</a></li>	  <li id="f-disclaimer"><a href="../../../g/e/n/Wikipedia%7EGeneral_disclaimer_3e44.html" title="Wikipedia:General disclaimer">Disclaimers</a></li>	  	</ul>
      </div>
    </div>
  </body>
</html>
